{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montando Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUY83epb08jF",
    "outputId": "17994811-470a-42bd-c070-8e16d594b245"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  %cd /content/drive/MyDrive/\n",
    "  %mkdir -p CasVit\n",
    "  %cd CasVit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7Z35vEKOv-w",
    "outputId": "d5fe730e-424c-42eb-8c5a-298bd66dae85"
   },
   "outputs": [],
   "source": [
    "\"\"\"Descomentar caso seja primeira vez usando notebook\"\"\"\n",
    "#!git clone https://github.com/johnnyharanaka/mmdetection.git\n",
    "#!git clone https://github.com/johnnyharanaka/CAS-ViT.git\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  %cd mmdetection\n",
    "  !pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "  !pip -q install openmim && mim install mmengine mmcv && pip install -e .\n",
    "  !pip install \"mmsegmentation>=1.0.0\" && pip install timm==1.0.9\n",
    "  !pip install tensorboardX\n",
    "  %cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "84-kMZXHQfLC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif 'google.colab' in sys.modules:\\n  !pip install gdown\\n  import gdown\\n  # Link do arquivo\\n  url = 'https://drive.google.com/uc?id=19l52ua_vvTtttgVRziCZJjal0TPE9f2p'\\n\\n\\n  # Caminho para salvar no Google Drive\\n  output = '/content/drive/MyDrive/CasVit/CAS-ViT/classification/data/data.zip'\\n\\n  # Fazer o download\\n  gdown.download(url, output, quiet=False)\\n  !unzip -q /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/data.zip -d /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "  HOME = '/content/drive/MyDrive'\n",
    "else:\n",
    "  HOME = '/Users/haranaka/Development/AI/RedesNeurais'\n",
    "\"\"\"\n",
    "if 'google.colab' in sys.modules:\n",
    "  !pip install gdown\n",
    "  import gdown\n",
    "  # Link do arquivo\n",
    "  #Caltech-UCSD Birds-200-2011\n",
    "  url = 'https://drive.google.com/uc?id=1mEKi61CNbla7KonRU1UYtN6NS9f5yHvc'\n",
    "  url2 = 'https://drive.google.com/uc?id=1bGuV5-gR2gSQOiaSB3T5zvTRBEFe1UTd'\n",
    "\n",
    "  # Caminho para salvar no Google Drive\n",
    "  output = '/content/drive/MyDrive/CasVit/CAS-ViT/classification/data/cub100.zip'\n",
    "  output = '/content/drive/MyDrive/CasVit/CAS-ViT/classification/data/meuDataset.zip'\n",
    "\n",
    "  # Fazer o download\n",
    "  gdown.download(url, output, quiet=False)\n",
    "  gdown.download(url2, output2, quiet=False)\n",
    "  !unzip -q /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/cub100.zip -d /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/\n",
    "  !unzip -q /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/meuDataset.zip -d /content/drive/MyDrive/CasVit/CAS-ViT/classification/data/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdUuAPCMy400",
    "outputId": "f49a435f-4937-4697-e895-ba2cd26b4feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/haranaka/Development/AI/RedesNeurais/CAS-ViT/classification\n"
     ]
    }
   ],
   "source": [
    "%cd CAS-ViT/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 bottleneck,\n",
    "                 dropout=0.2,\n",
    "                 init_option=\"bert\",\n",
    "                 adapter_scalar=\"1.0\",\n",
    "                 adapter_layernorm_option=\"out\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = d_model\n",
    "        self.down_size = bottleneck\n",
    "        \n",
    "        self.adapter_layernorm_option = adapter_layernorm_option\n",
    "\n",
    "        self.adapter_layer_norm_before = None\n",
    "        if adapter_layernorm_option == \"in\" or adapter_layernorm_option == \"out\":\n",
    "            #self.adapter_layer_norm_before = nn.InstanceNorm2d(self.n_embd,affine=True)\n",
    "            self.adapter_layer_norm_before = nn.BatchNorm2d(self.n_embd)\n",
    "\n",
    "        if adapter_scalar == \"learnable_scalar\":\n",
    "            self.scale = nn.Parameter(torch.ones(1))\n",
    "        else:\n",
    "            self.scale = float(adapter_scalar)\n",
    "\n",
    "        self.down_proj = nn.Conv2d(self.n_embd, self.down_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.non_linear_func = nn.ReLU()\n",
    "\n",
    "        #self.up_proj = nn.ConvTranspose2d(self.down_size, self.n_embd, kernel_size=3, stride=1, padding=1)\n",
    "        self.up_proj = nn.Conv2d(self.down_size, self.n_embd, kernel_size=1, stride=1, padding=0)\n",
    "        #self.up_proj = nn.Conv2d(self.down_size, self.n_embd, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        if init_option == \"bert\":\n",
    "            raise NotImplementedError\n",
    "        elif init_option == \"lora\":\n",
    "            with torch.no_grad():\n",
    "                nn.init.kaiming_uniform_(self.down_proj.weight, a=math.sqrt(5))\n",
    "                nn.init.zeros_(self.up_proj.weight)\n",
    "                nn.init.zeros_(self.down_proj.bias)\n",
    "                nn.init.zeros_(self.up_proj.bias)\n",
    "\n",
    "    def forward(self, x, add_residual=False, residual=None):\n",
    "\n",
    "        residual = x if residual is None else residual\n",
    "        if self.adapter_layernorm_option == 'in':\n",
    "            x = self.adapter_layer_norm_before(x)\n",
    "\n",
    "        down = self.down_proj(x)\n",
    "        down = self.non_linear_func(down)\n",
    "        down = nn.functional.dropout(down, p=self.dropout, training=self.training)\n",
    "        up = self.up_proj(down)\n",
    "\n",
    "        up = up * self.scale\n",
    "        \n",
    "        if self.adapter_layernorm_option == 'out':\n",
    "            up = self.adapter_layer_norm_before(up)\n",
    "\n",
    "        if add_residual:\n",
    "            output = up + residual\n",
    "        else:\n",
    "            output = up\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Arquitetura do CAS-Vit</h1>\n",
    "<img src=\"https://github.com/Tianfang-Zhang/CAS-ViT/blob/main/assets/arch.png?raw=true\" alt=\"Exemplo de Imagem\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino dos Modelos no Meu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZGd715ju40d",
    "outputId": "2e292286-1f05-431b-c0ba-cc209af3e8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 3\n",
      "Warping 384 size input images...\n",
      "Number of the class = 3\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x3272e8e50>\n",
      "log writter dir:  None\n",
      "size mismatch for head.weight: copying a param with shape torch.Size([1000, 220]) from checkpoint, the shape in current model is torch.Size([3, 220]).\n",
      "size mismatch for head.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Finetune resume checkpoint: model/cas-vit-xs.pth\n",
      "number of params: 2759363\n",
      "LR = 0.00005000\n",
      "Batch size = 64\n",
      "Update frequent = 2\n",
      "Number of training examples = 595\n",
      "Number of training training per epoch = 9\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Use Cosine LR scheduler\n",
      "Set warmup steps = 27\n",
      "epochs: 20, niter_per_ep: 9,  len(schedule): 180\n",
      "Set warmup steps = 0\n",
      "epochs: 20, niter_per_ep: 9,  len(schedule): 180\n",
      "Max WD = 0.0500000, Min WD = 0.0500000\n",
      "criterion = LabelSmoothingCrossEntropy()\n",
      "Auto resume checkpoint: \n",
      "Total Trainable Params: 2759363\n",
      "Start training for 20 epochs\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch: [0]  [ 0/18]  eta: 0:00:48  lr: 0.000000  min_lr: 0.000000  loss: 1.2940 (1.2940)  class_acc: 0.2500 (0.2500)  weight_decay: 0.0500 (0.0500)  time: 2.6933  data: 0.2954\n",
      "Epoch: [0]  [17/18]  eta: 0:00:01  lr: 0.000015  min_lr: 0.000015  loss: 1.2264 (1.2339)  class_acc: 0.3125 (0.3507)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2264 (5.5988)  time: 1.0083  data: 0.2620\n",
      "Epoch: [0] Total time: 0:00:18 (1.0084 s / it)\n",
      "Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 1.2264 (1.2339)  class_acc: 0.3125 (0.3507)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2264 (5.5988)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 1.1301 (1.1301)  acc1: 16.6667 (16.6667)  acc5: 100.0000 (100.0000)  time: 1.1086  data: 0.4225\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.1301 (1.1152)  acc1: 16.6667 (33.0709)  acc5: 100.0000 (100.0000)  time: 0.7725  data: 0.3667\n",
      "Test: Total time: 0:00:02 (0.7727 s / it)\n",
      "* Acc@1 33.071 Acc@5 100.000 loss 1.115\n",
      "Accuracy of the model on the 127 test images: 33.1%\n",
      "Epoch: [1]  [ 0/18]  eta: 0:00:16  lr: 0.000017  min_lr: 0.000017  loss: 1.3486 (1.3486)  class_acc: 0.3125 (0.3125)  weight_decay: 0.0500 (0.0500)  time: 0.8944  data: 0.2936\n",
      "Epoch: [1]  [17/18]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000033  loss: 1.1690 (1.1838)  class_acc: 0.3125 (0.3316)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7588 (5.4777)  time: 0.8964  data: 0.2662\n",
      "Epoch: [1] Total time: 0:00:16 (0.8964 s / it)\n",
      "Averaged stats: lr: 0.000033  min_lr: 0.000033  loss: 1.1690 (1.1838)  class_acc: 0.3125 (0.3316)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7588 (5.4777)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 1.0695 (1.0695)  acc1: 14.5833 (14.5833)  acc5: 100.0000 (100.0000)  time: 1.0205  data: 0.4623\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0695 (1.1462)  acc1: 14.5833 (32.2835)  acc5: 100.0000 (100.0000)  time: 0.6996  data: 0.3838\n",
      "Test: Total time: 0:00:02 (0.6998 s / it)\n",
      "* Acc@1 32.283 Acc@5 100.000 loss 1.146\n",
      "Accuracy of the model on the 127 test images: 32.3%\n",
      "Epoch: [2]  [ 0/18]  eta: 0:00:15  lr: 0.000035  min_lr: 0.000035  loss: 1.1191 (1.1191)  class_acc: 0.3750 (0.3750)  weight_decay: 0.0500 (0.0500)  time: 0.8593  data: 0.2700\n",
      "Epoch: [2]  [17/18]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000050  loss: 1.1191 (1.1249)  class_acc: 0.3438 (0.3420)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2059 (4.1549)  time: 0.8855  data: 0.2607\n",
      "Epoch: [2] Total time: 0:00:15 (0.8855 s / it)\n",
      "Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 1.1191 (1.1249)  class_acc: 0.3438 (0.3420)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2059 (4.1549)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.0374 (1.0374)  acc1: 14.5833 (14.5833)  acc5: 100.0000 (100.0000)  time: 0.9841  data: 0.4402\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0374 (1.1681)  acc1: 14.5833 (32.2835)  acc5: 100.0000 (100.0000)  time: 0.6831  data: 0.3724\n",
      "Test: Total time: 0:00:02 (0.6833 s / it)\n",
      "* Acc@1 32.283 Acc@5 100.000 loss 1.168\n",
      "Accuracy of the model on the 127 test images: 32.3%\n",
      "Epoch: [3]  [ 0/18]  eta: 0:00:15  lr: 0.000050  min_lr: 0.000050  loss: 1.1058 (1.1058)  class_acc: 0.4375 (0.4375)  weight_decay: 0.0500 (0.0500)  time: 0.8681  data: 0.2774\n",
      "Epoch: [3]  [17/18]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000050  loss: 1.0889 (1.0987)  class_acc: 0.5000 (0.4792)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2453 (4.2384)  time: 0.8946  data: 0.2693\n",
      "Epoch: [3] Total time: 0:00:16 (0.8946 s / it)\n",
      "Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 1.0889 (1.0987)  class_acc: 0.5000 (0.4792)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2453 (4.2384)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 1.0055 (1.0055)  acc1: 14.5833 (14.5833)  acc5: 100.0000 (100.0000)  time: 1.0741  data: 0.4297\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0055 (1.1499)  acc1: 14.5833 (32.2835)  acc5: 100.0000 (100.0000)  time: 0.7258  data: 0.3795\n",
      "Test: Total time: 0:00:02 (0.7260 s / it)\n",
      "* Acc@1 32.283 Acc@5 100.000 loss 1.150\n",
      "Accuracy of the model on the 127 test images: 32.3%\n",
      "Epoch: [4]  [ 0/18]  eta: 0:00:15  lr: 0.000050  min_lr: 0.000050  loss: 1.0651 (1.0651)  class_acc: 0.5000 (0.5000)  weight_decay: 0.0500 (0.0500)  time: 0.8707  data: 0.2820\n",
      "Epoch: [4]  [17/18]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000049  loss: 1.0673 (1.0884)  class_acc: 0.4688 (0.4792)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6596 (4.5214)  time: 0.8890  data: 0.2599\n",
      "Epoch: [4] Total time: 0:00:16 (0.8890 s / it)\n",
      "Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 1.0673 (1.0884)  class_acc: 0.4688 (0.4792)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6596 (4.5214)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.0605 (1.0605)  acc1: 14.5833 (14.5833)  acc5: 100.0000 (100.0000)  time: 0.9410  data: 0.4302\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0605 (1.0743)  acc1: 14.5833 (32.2835)  acc5: 100.0000 (100.0000)  time: 0.6680  data: 0.3679\n",
      "Test: Total time: 0:00:02 (0.6681 s / it)\n",
      "* Acc@1 32.283 Acc@5 100.000 loss 1.074\n",
      "Accuracy of the model on the 127 test images: 32.3%\n",
      "Epoch: [5]  [ 0/18]  eta: 0:00:15  lr: 0.000049  min_lr: 0.000049  loss: 1.0704 (1.0704)  class_acc: 0.5000 (0.5000)  weight_decay: 0.0500 (0.0500)  time: 0.8663  data: 0.2795\n",
      "Epoch: [5]  [17/18]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000047  loss: 1.0375 (1.0372)  class_acc: 0.6250 (0.5920)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8223 (3.5829)  time: 0.8850  data: 0.2653\n",
      "Epoch: [5] Total time: 0:00:15 (0.8851 s / it)\n",
      "Averaged stats: lr: 0.000047  min_lr: 0.000047  loss: 1.0375 (1.0372)  class_acc: 0.6250 (0.5920)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8223 (3.5829)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.0401 (1.0401)  acc1: 37.5000 (37.5000)  acc5: 100.0000 (100.0000)  time: 0.6726  data: 0.4164\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0401 (1.0210)  acc1: 37.5000 (47.2441)  acc5: 100.0000 (100.0000)  time: 0.5717  data: 0.3599\n",
      "Test: Total time: 0:00:01 (0.5718 s / it)\n",
      "* Acc@1 47.244 Acc@5 100.000 loss 1.021\n",
      "Accuracy of the model on the 127 test images: 47.2%\n",
      "Epoch: [6]  [ 0/18]  eta: 0:00:15  lr: 0.000047  min_lr: 0.000047  loss: 0.9800 (0.9800)  class_acc: 0.7500 (0.7500)  weight_decay: 0.0500 (0.0500)  time: 0.8423  data: 0.2489\n",
      "Epoch: [6]  [17/18]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000045  loss: 1.0254 (1.0319)  class_acc: 0.5625 (0.6007)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9135 (3.7025)  time: 0.8846  data: 0.2664\n",
      "Epoch: [6] Total time: 0:00:15 (0.8846 s / it)\n",
      "Averaged stats: lr: 0.000045  min_lr: 0.000045  loss: 1.0254 (1.0319)  class_acc: 0.5625 (0.6007)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9135 (3.7025)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.0092 (1.0092)  acc1: 41.6667 (41.6667)  acc5: 100.0000 (100.0000)  time: 0.6861  data: 0.4270\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0092 (0.9865)  acc1: 41.6667 (59.8425)  acc5: 100.0000 (100.0000)  time: 0.5806  data: 0.3670\n",
      "Test: Total time: 0:00:01 (0.5808 s / it)\n",
      "* Acc@1 59.843 Acc@5 100.000 loss 0.986\n",
      "Accuracy of the model on the 127 test images: 59.8%\n",
      "Epoch: [7]  [ 0/18]  eta: 0:00:15  lr: 0.000045  min_lr: 0.000045  loss: 1.0859 (1.0859)  class_acc: 0.4688 (0.4688)  weight_decay: 0.0500 (0.0500)  time: 0.8392  data: 0.2473\n",
      "Epoch: [7]  [17/18]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000042  loss: 1.0025 (1.0039)  class_acc: 0.6250 (0.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1975 (3.8723)  time: 0.9001  data: 0.2762\n",
      "Epoch: [7] Total time: 0:00:16 (0.9001 s / it)\n",
      "Averaged stats: lr: 0.000042  min_lr: 0.000042  loss: 1.0025 (1.0039)  class_acc: 0.6250 (0.6337)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1975 (3.8723)\n",
      "Test:  [0/3]  eta: 0:00:04  loss: 0.9419 (0.9419)  acc1: 54.1667 (54.1667)  acc5: 100.0000 (100.0000)  time: 1.5913  data: 0.4547\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.9419 (0.9677)  acc1: 54.1667 (63.7795)  acc5: 100.0000 (100.0000)  time: 0.9134  data: 0.3975\n",
      "Test: Total time: 0:00:02 (0.9136 s / it)\n",
      "* Acc@1 63.780 Acc@5 100.000 loss 0.968\n",
      "Accuracy of the model on the 127 test images: 63.8%\n",
      "Epoch: [8]  [ 0/18]  eta: 0:00:16  lr: 0.000042  min_lr: 0.000042  loss: 0.9630 (0.9630)  class_acc: 0.6562 (0.6562)  weight_decay: 0.0500 (0.0500)  time: 0.8902  data: 0.2762\n",
      "Epoch: [8]  [17/18]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000039  loss: 0.9771 (0.9721)  class_acc: 0.6250 (0.6615)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7925 (3.8052)  time: 0.8989  data: 0.2751\n",
      "Epoch: [8] Total time: 0:00:16 (0.8990 s / it)\n",
      "Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 0.9771 (0.9721)  class_acc: 0.6250 (0.6615)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7925 (3.8052)\n",
      "Test:  [0/3]  eta: 0:00:04  loss: 0.8918 (0.8918)  acc1: 72.9167 (72.9167)  acc5: 100.0000 (100.0000)  time: 1.6663  data: 0.5367\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.8918 (0.9350)  acc1: 72.9167 (76.3780)  acc5: 100.0000 (100.0000)  time: 0.9986  data: 0.4751\n",
      "Test: Total time: 0:00:02 (0.9994 s / it)\n",
      "* Acc@1 76.378 Acc@5 100.000 loss 0.935\n",
      "Accuracy of the model on the 127 test images: 76.4%\n",
      "Epoch: [9]  [ 0/18]  eta: 0:00:16  lr: 0.000039  min_lr: 0.000039  loss: 0.9776 (0.9776)  class_acc: 0.7188 (0.7188)  weight_decay: 0.0500 (0.0500)  time: 0.9011  data: 0.3031\n",
      "Epoch: [9]  [17/18]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000036  loss: 0.9285 (0.9356)  class_acc: 0.7188 (0.7188)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5096 (3.3784)  time: 0.9064  data: 0.2707\n",
      "Epoch: [9] Total time: 0:00:16 (0.9065 s / it)\n",
      "Averaged stats: lr: 0.000036  min_lr: 0.000036  loss: 0.9285 (0.9356)  class_acc: 0.7188 (0.7188)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5096 (3.3784)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 0.8644 (0.8644)  acc1: 77.0833 (77.0833)  acc5: 100.0000 (100.0000)  time: 1.0286  data: 0.4847\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.8644 (0.8891)  acc1: 77.0833 (82.6772)  acc5: 100.0000 (100.0000)  time: 0.6948  data: 0.3859\n",
      "Test: Total time: 0:00:02 (0.6950 s / it)\n",
      "* Acc@1 82.677 Acc@5 100.000 loss 0.889\n",
      "Accuracy of the model on the 127 test images: 82.7%\n",
      "Epoch: [10]  [ 0/18]  eta: 0:00:15  lr: 0.000035  min_lr: 0.000035  loss: 0.9350 (0.9350)  class_acc: 0.6562 (0.6562)  weight_decay: 0.0500 (0.0500)  time: 0.8543  data: 0.2598\n",
      "Epoch: [10]  [17/18]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000032  loss: 0.9000 (0.9087)  class_acc: 0.7188 (0.7396)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3592 (3.3080)  time: 0.8860  data: 0.2649\n",
      "Epoch: [10] Total time: 0:00:15 (0.8861 s / it)\n",
      "Averaged stats: lr: 0.000032  min_lr: 0.000032  loss: 0.9000 (0.9087)  class_acc: 0.7188 (0.7396)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3592 (3.3080)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 0.8766 (0.8766)  acc1: 64.5833 (64.5833)  acc5: 100.0000 (100.0000)  time: 1.0338  data: 0.4784\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.8766 (0.8650)  acc1: 64.5833 (77.1654)  acc5: 100.0000 (100.0000)  time: 0.6996  data: 0.3879\n",
      "Test: Total time: 0:00:02 (0.6997 s / it)\n",
      "* Acc@1 77.165 Acc@5 100.000 loss 0.865\n",
      "Accuracy of the model on the 127 test images: 77.2%\n",
      "Epoch: [11]  [ 0/18]  eta: 0:00:15  lr: 0.000032  min_lr: 0.000032  loss: 0.9119 (0.9119)  class_acc: 0.7500 (0.7500)  weight_decay: 0.0500 (0.0500)  time: 0.8427  data: 0.2571\n",
      "Epoch: [11]  [17/18]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000029  loss: 0.8862 (0.8839)  class_acc: 0.7500 (0.7448)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2657 (3.1934)  time: 0.8862  data: 0.2605\n",
      "Epoch: [11] Total time: 0:00:15 (0.8862 s / it)\n",
      "Averaged stats: lr: 0.000029  min_lr: 0.000029  loss: 0.8862 (0.8839)  class_acc: 0.7500 (0.7448)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2657 (3.1934)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.8272 (0.8272)  acc1: 72.9167 (72.9167)  acc5: 100.0000 (100.0000)  time: 0.9872  data: 0.4159\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.8272 (0.8171)  acc1: 72.9167 (80.3150)  acc5: 100.0000 (100.0000)  time: 0.6845  data: 0.3650\n",
      "Test: Total time: 0:00:02 (0.6847 s / it)\n",
      "* Acc@1 80.315 Acc@5 100.000 loss 0.817\n",
      "Accuracy of the model on the 127 test images: 80.3%\n",
      "Epoch: [12]  [ 0/18]  eta: 0:00:15  lr: 0.000028  min_lr: 0.000028  loss: 0.7689 (0.7689)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 0.8831  data: 0.2898\n",
      "Epoch: [12]  [17/18]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000025  loss: 0.8604 (0.8513)  class_acc: 0.7812 (0.7726)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5916 (3.6123)  time: 0.8711  data: 0.2456\n",
      "Epoch: [12] Total time: 0:00:15 (0.8711 s / it)\n",
      "Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 0.8604 (0.8513)  class_acc: 0.7812 (0.7726)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5916 (3.6123)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 0.7810 (0.7810)  acc1: 75.0000 (75.0000)  acc5: 100.0000 (100.0000)  time: 1.1545  data: 0.4786\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.7810 (0.8084)  acc1: 75.0000 (81.1024)  acc5: 100.0000 (100.0000)  time: 0.7490  data: 0.3895\n",
      "Test: Total time: 0:00:02 (0.7492 s / it)\n",
      "* Acc@1 81.102 Acc@5 100.000 loss 0.808\n",
      "Accuracy of the model on the 127 test images: 81.1%\n",
      "Epoch: [13]  [ 0/18]  eta: 0:00:15  lr: 0.000025  min_lr: 0.000025  loss: 0.8030 (0.8030)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 0.8809  data: 0.2766\n",
      "Epoch: [13]  [17/18]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000021  loss: 0.8211 (0.8341)  class_acc: 0.7500 (0.7604)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2739 (3.4547)  time: 0.8848  data: 0.2511\n",
      "Epoch: [13] Total time: 0:00:15 (0.8849 s / it)\n",
      "Averaged stats: lr: 0.000021  min_lr: 0.000021  loss: 0.8211 (0.8341)  class_acc: 0.7500 (0.7604)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2739 (3.4547)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.7400 (0.7400)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.9582  data: 0.4112\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.7400 (0.7595)  acc1: 81.2500 (83.4646)  acc5: 100.0000 (100.0000)  time: 0.6801  data: 0.3648\n",
      "Test: Total time: 0:00:02 (0.6803 s / it)\n",
      "* Acc@1 83.465 Acc@5 100.000 loss 0.759\n",
      "Accuracy of the model on the 127 test images: 83.5%\n",
      "Epoch: [14]  [ 0/18]  eta: 0:00:15  lr: 0.000021  min_lr: 0.000021  loss: 0.7997 (0.7997)  class_acc: 0.8438 (0.8438)  weight_decay: 0.0500 (0.0500)  time: 0.8725  data: 0.2613\n",
      "Epoch: [14]  [17/18]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.8261 (0.8135)  class_acc: 0.7812 (0.7795)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6187 (3.4109)  time: 0.8840  data: 0.2517\n",
      "Epoch: [14] Total time: 0:00:15 (0.8840 s / it)\n",
      "Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 0.8261 (0.8135)  class_acc: 0.7812 (0.7795)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6187 (3.4109)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.6995 (0.6995)  acc1: 81.2500 (81.2500)  acc5: 100.0000 (100.0000)  time: 0.9863  data: 0.4369\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6995 (0.7235)  acc1: 81.2500 (84.2520)  acc5: 100.0000 (100.0000)  time: 0.6747  data: 0.3658\n",
      "Test: Total time: 0:00:02 (0.6748 s / it)\n",
      "* Acc@1 84.252 Acc@5 100.000 loss 0.723\n",
      "Accuracy of the model on the 127 test images: 84.3%\n",
      "Epoch: [15]  [ 0/18]  eta: 0:00:15  lr: 0.000018  min_lr: 0.000018  loss: 0.8165 (0.8165)  class_acc: 0.7500 (0.7500)  weight_decay: 0.0500 (0.0500)  time: 0.8542  data: 0.2605\n",
      "Epoch: [15]  [17/18]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000015  loss: 0.7917 (0.7971)  class_acc: 0.7812 (0.7726)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1728 (3.0569)  time: 0.8692  data: 0.2447\n",
      "Epoch: [15] Total time: 0:00:15 (0.8693 s / it)\n",
      "Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 0.7917 (0.7971)  class_acc: 0.7812 (0.7726)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1728 (3.0569)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.6819 (0.6819)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 0.8569  data: 0.4078\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6819 (0.6951)  acc1: 87.5000 (86.6142)  acc5: 100.0000 (100.0000)  time: 0.6307  data: 0.3546\n",
      "Test: Total time: 0:00:01 (0.6309 s / it)\n",
      "* Acc@1 86.614 Acc@5 100.000 loss 0.695\n",
      "Accuracy of the model on the 127 test images: 86.6%\n",
      "Epoch: [16]  [ 0/18]  eta: 0:00:15  lr: 0.000015  min_lr: 0.000015  loss: 0.9136 (0.9136)  class_acc: 0.5625 (0.5625)  weight_decay: 0.0500 (0.0500)  time: 0.8372  data: 0.2437\n",
      "Epoch: [16]  [17/18]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000013  loss: 0.7931 (0.7943)  class_acc: 0.7812 (0.7847)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9615 (3.0521)  time: 0.8740  data: 0.2492\n",
      "Epoch: [16] Total time: 0:00:15 (0.8740 s / it)\n",
      "Averaged stats: lr: 0.000013  min_lr: 0.000013  loss: 0.7931 (0.7943)  class_acc: 0.7812 (0.7847)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9615 (3.0521)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.6531 (0.6531)  acc1: 85.4167 (85.4167)  acc5: 100.0000 (100.0000)  time: 0.6680  data: 0.4103\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6531 (0.6581)  acc1: 85.4167 (86.6142)  acc5: 100.0000 (100.0000)  time: 0.5616  data: 0.3500\n",
      "Test: Total time: 0:00:01 (0.5617 s / it)\n",
      "* Acc@1 86.614 Acc@5 100.000 loss 0.658\n",
      "Accuracy of the model on the 127 test images: 86.6%\n",
      "Epoch: [17]  [ 0/18]  eta: 0:00:14  lr: 0.000013  min_lr: 0.000013  loss: 0.7935 (0.7935)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 0.8210  data: 0.2355\n",
      "Epoch: [17]  [17/18]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000012  loss: 0.7738 (0.7755)  class_acc: 0.7812 (0.7899)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0585 (3.1942)  time: 0.8903  data: 0.2605\n",
      "Epoch: [17] Total time: 0:00:16 (0.8904 s / it)\n",
      "Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 0.7738 (0.7755)  class_acc: 0.7812 (0.7899)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0585 (3.1942)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.6592 (0.6592)  acc1: 83.3333 (83.3333)  acc5: 100.0000 (100.0000)  time: 0.9925  data: 0.4297\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6592 (0.6589)  acc1: 83.3333 (85.0394)  acc5: 100.0000 (100.0000)  time: 0.6911  data: 0.3734\n",
      "Test: Total time: 0:00:02 (0.6912 s / it)\n",
      "* Acc@1 85.039 Acc@5 100.000 loss 0.659\n",
      "Accuracy of the model on the 127 test images: 85.0%\n",
      "Epoch: [18]  [ 0/18]  eta: 0:00:16  lr: 0.000011  min_lr: 0.000011  loss: 0.7884 (0.7884)  class_acc: 0.7812 (0.7812)  weight_decay: 0.0500 (0.0500)  time: 0.9253  data: 0.3154\n",
      "Epoch: [18]  [17/18]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000010  loss: 0.7523 (0.7534)  class_acc: 0.7812 (0.7882)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0561 (3.0453)  time: 0.9577  data: 0.3155\n",
      "Epoch: [18] Total time: 0:00:17 (0.9578 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 0.7523 (0.7534)  class_acc: 0.7812 (0.7882)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0561 (3.0453)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 0.6238 (0.6238)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 1.1782  data: 0.4992\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6238 (0.6402)  acc1: 87.5000 (86.6142)  acc5: 100.0000 (100.0000)  time: 0.7580  data: 0.3987\n",
      "Test: Total time: 0:00:02 (0.7582 s / it)\n",
      "* Acc@1 86.614 Acc@5 100.000 loss 0.640\n",
      "Accuracy of the model on the 127 test images: 86.6%\n",
      "Epoch: [19]  [ 0/18]  eta: 0:00:15  lr: 0.000010  min_lr: 0.000010  loss: 0.7322 (0.7322)  class_acc: 0.7500 (0.7500)  weight_decay: 0.0500 (0.0500)  time: 0.8644  data: 0.2625\n",
      "Epoch: [19]  [17/18]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000010  loss: 0.7471 (0.7436)  class_acc: 0.7812 (0.7917)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0436 (2.9166)  time: 0.9322  data: 0.3023\n",
      "Epoch: [19] Total time: 0:00:16 (0.9323 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 0.7471 (0.7436)  class_acc: 0.7812 (0.7917)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.0436 (2.9166)\n",
      "Test:  [0/3]  eta: 0:00:03  loss: 0.6082 (0.6082)  acc1: 87.5000 (87.5000)  acc5: 100.0000 (100.0000)  time: 1.1819  data: 0.4972\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.6082 (0.6283)  acc1: 87.5000 (86.6142)  acc5: 100.0000 (100.0000)  time: 0.7595  data: 0.3964\n",
      "Test: Total time: 0:00:02 (0.7597 s / it)\n",
      "* Acc@1 86.614 Acc@5 100.000 loss 0.628\n",
      "Accuracy of the model on the 127 test images: 86.6%\n",
      "Training time 0:06:14\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 32 --epochs 20 --model rcvit_xs --data_path data/meuDataset/ --lr 5e-5 --min_lr 1e-5 --weight_decay 0.05 --nb_classes 3 --output_dir data/res_fine --finetune model/cas-vit-xs.pth --warmup_epochs 3 --data_set image_folder --model_ema_eval False --model_ema False --device {device} --adapter False --input_size 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 3\n",
      "Warping 384 size input images...\n",
      "Number of the class = 3\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x324c9ce20>\n",
      "log writter dir:  None\n",
      "Weights of RCViT_adapter not initialized from pretrained model: ['cur_adapter.0.adapter_layer_norm_before.weight', 'cur_adapter.0.adapter_layer_norm_before.bias', 'cur_adapter.0.adapter_layer_norm_before.running_mean', 'cur_adapter.0.adapter_layer_norm_before.running_var', 'cur_adapter.0.down_proj.weight', 'cur_adapter.0.down_proj.bias', 'cur_adapter.0.up_proj.weight', 'cur_adapter.0.up_proj.bias', 'cur_adapter.1.adapter_layer_norm_before.weight', 'cur_adapter.1.adapter_layer_norm_before.bias', 'cur_adapter.1.adapter_layer_norm_before.running_mean', 'cur_adapter.1.adapter_layer_norm_before.running_var', 'cur_adapter.1.down_proj.weight', 'cur_adapter.1.down_proj.bias', 'cur_adapter.1.up_proj.weight', 'cur_adapter.1.up_proj.bias', 'cur_adapter.2.adapter_layer_norm_before.weight', 'cur_adapter.2.adapter_layer_norm_before.bias', 'cur_adapter.2.adapter_layer_norm_before.running_mean', 'cur_adapter.2.adapter_layer_norm_before.running_var', 'cur_adapter.2.down_proj.weight', 'cur_adapter.2.down_proj.bias', 'cur_adapter.2.up_proj.weight', 'cur_adapter.2.up_proj.bias', 'cur_adapter.3.adapter_layer_norm_before.weight', 'cur_adapter.3.adapter_layer_norm_before.bias', 'cur_adapter.3.adapter_layer_norm_before.running_mean', 'cur_adapter.3.adapter_layer_norm_before.running_var', 'cur_adapter.3.down_proj.weight', 'cur_adapter.3.down_proj.bias', 'cur_adapter.3.up_proj.weight', 'cur_adapter.3.up_proj.bias', 'cur_adapter.4.adapter_layer_norm_before.weight', 'cur_adapter.4.adapter_layer_norm_before.bias', 'cur_adapter.4.adapter_layer_norm_before.running_mean', 'cur_adapter.4.adapter_layer_norm_before.running_var', 'cur_adapter.4.down_proj.weight', 'cur_adapter.4.down_proj.bias', 'cur_adapter.4.up_proj.weight', 'cur_adapter.4.up_proj.bias', 'cur_adapter.5.adapter_layer_norm_before.weight', 'cur_adapter.5.adapter_layer_norm_before.bias', 'cur_adapter.5.adapter_layer_norm_before.running_mean', 'cur_adapter.5.adapter_layer_norm_before.running_var', 'cur_adapter.5.down_proj.weight', 'cur_adapter.5.down_proj.bias', 'cur_adapter.5.up_proj.weight', 'cur_adapter.5.up_proj.bias', 'cur_adapter.6.adapter_layer_norm_before.weight', 'cur_adapter.6.adapter_layer_norm_before.bias', 'cur_adapter.6.adapter_layer_norm_before.running_mean', 'cur_adapter.6.adapter_layer_norm_before.running_var', 'cur_adapter.6.down_proj.weight', 'cur_adapter.6.down_proj.bias', 'cur_adapter.6.up_proj.weight', 'cur_adapter.6.up_proj.bias']\n",
      "size mismatch for head.weight: copying a param with shape torch.Size([1000, 220]) from checkpoint, the shape in current model is torch.Size([3, 220]).\n",
      "size mismatch for head.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "Finetune resume checkpoint: model/cas-vit-xs.pth\n",
      "number of params: 134011\n",
      "Cas-Vit with Adapters ----------------------\n",
      "LR = 0.00050000\n",
      "Batch size = 64\n",
      "Update frequent = 2\n",
      "Number of training examples = 595\n",
      "Number of training training per epoch = 9\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Use Cosine LR scheduler\n",
      "Set warmup steps = 9\n",
      "epochs: 20, niter_per_ep: 9,  len(schedule): 180\n",
      "Set warmup steps = 0\n",
      "epochs: 20, niter_per_ep: 9,  len(schedule): 180\n",
      "Max WD = 0.5000000, Min WD = 0.5000000\n",
      "criterion = LabelSmoothingCrossEntropy()\n",
      "Auto resume checkpoint: \n",
      "Total Trainable Params: 134011\n",
      "Start training for 20 epochs\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch: [0]  [ 0/18]  eta: 0:00:18  lr: 0.000000  min_lr: 0.000000  loss: 1.2092 (1.2092)  class_acc: 0.4062 (0.4062)  weight_decay: 0.5000 (0.5000)  time: 1.0509  data: 0.2864\n",
      "Epoch: [0]  [17/18]  eta: 0:00:00  lr: 0.000500  min_lr: 0.000500  loss: 1.1779 (1.1487)  class_acc: 0.5000 (0.5365)  weight_decay: 0.5000 (0.5000)  grad_norm: 137.8706 (122.7396)  time: 0.6173  data: 0.2598\n",
      "Epoch: [0] Total time: 0:00:11 (0.6174 s / it)\n",
      "Averaged stats: lr: 0.000500  min_lr: 0.000500  loss: 1.1779 (1.1487)  class_acc: 0.5000 (0.5365)  weight_decay: 0.5000 (0.5000)  grad_norm: 137.8706 (122.7396)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.1724 (1.1724)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 0.8362  data: 0.4128\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0691 (1.0858)  acc1: 29.1667 (35.4331)  acc5: 100.0000 (100.0000)  time: 0.6673  data: 0.3577\n",
      "Test: Total time: 0:00:02 (0.6675 s / it)\n",
      "* Acc@1 35.433 Acc@5 100.000 loss 1.086\n",
      "Accuracy of the model on the 127 test images: 35.4%\n",
      "Epoch: [1]  [ 0/18]  eta: 0:00:10  lr: 0.000500  min_lr: 0.000500  loss: 0.9861 (0.9861)  class_acc: 0.6250 (0.6250)  weight_decay: 0.5000 (0.5000)  time: 0.5947  data: 0.2704\n",
      "Epoch: [1]  [17/18]  eta: 0:00:00  lr: 0.000497  min_lr: 0.000497  loss: 0.7572 (0.7494)  class_acc: 0.7188 (0.7361)  weight_decay: 0.5000 (0.5000)  grad_norm: 30.1962 (32.4624)  time: 0.5756  data: 0.2552\n",
      "Epoch: [1] Total time: 0:00:10 (0.5756 s / it)\n",
      "Averaged stats: lr: 0.000497  min_lr: 0.000497  loss: 0.7572 (0.7494)  class_acc: 0.7188 (0.7361)  weight_decay: 0.5000 (0.5000)  grad_norm: 30.1962 (32.4624)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 1.2563 (1.2563)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 0.6482  data: 0.4023\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0355 (1.0844)  acc1: 29.1667 (35.4331)  acc5: 100.0000 (100.0000)  time: 0.5545  data: 0.3442\n",
      "Test: Total time: 0:00:01 (0.5546 s / it)\n",
      "* Acc@1 35.433 Acc@5 100.000 loss 1.084\n",
      "Accuracy of the model on the 127 test images: 35.4%\n",
      "Epoch: [2]  [ 0/18]  eta: 0:00:10  lr: 0.000497  min_lr: 0.000497  loss: 0.6097 (0.6097)  class_acc: 0.7812 (0.7812)  weight_decay: 0.5000 (0.5000)  time: 0.5600  data: 0.2455\n",
      "Epoch: [2]  [17/18]  eta: 0:00:00  lr: 0.000488  min_lr: 0.000488  loss: 0.6097 (0.6078)  class_acc: 0.8438 (0.8507)  weight_decay: 0.5000 (0.5000)  grad_norm: 13.9542 (14.0179)  time: 0.5775  data: 0.2597\n",
      "Epoch: [2] Total time: 0:00:10 (0.5776 s / it)\n",
      "Averaged stats: lr: 0.000488  min_lr: 0.000488  loss: 0.6097 (0.6078)  class_acc: 0.8438 (0.8507)  weight_decay: 0.5000 (0.5000)  grad_norm: 13.9542 (14.0179)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.2386 (1.2386)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 0.6757  data: 0.4323\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.0515 (1.0773)  acc1: 29.1667 (35.4331)  acc5: 100.0000 (100.0000)  time: 0.5679  data: 0.3578\n",
      "Test: Total time: 0:00:01 (0.5681 s / it)\n",
      "* Acc@1 35.433 Acc@5 100.000 loss 1.077\n",
      "Accuracy of the model on the 127 test images: 35.4%\n",
      "Epoch: [3]  [ 0/18]  eta: 0:00:10  lr: 0.000487  min_lr: 0.000487  loss: 0.6323 (0.6323)  class_acc: 0.8125 (0.8125)  weight_decay: 0.5000 (0.5000)  time: 0.5606  data: 0.2424\n",
      "Epoch: [3]  [17/18]  eta: 0:00:00  lr: 0.000473  min_lr: 0.000473  loss: 0.5305 (0.5538)  class_acc: 0.8750 (0.8663)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.9268 (11.7305)  time: 0.5792  data: 0.2575\n",
      "Epoch: [3] Total time: 0:00:10 (0.5793 s / it)\n",
      "Averaged stats: lr: 0.000473  min_lr: 0.000473  loss: 0.5305 (0.5538)  class_acc: 0.8750 (0.8663)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.9268 (11.7305)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 1.1587 (1.1587)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 0.6575  data: 0.4108\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 1.1587 (1.0525)  acc1: 29.1667 (35.4331)  acc5: 100.0000 (100.0000)  time: 0.5655  data: 0.3544\n",
      "Test: Total time: 0:00:01 (0.5657 s / it)\n",
      "* Acc@1 35.433 Acc@5 100.000 loss 1.052\n",
      "Accuracy of the model on the 127 test images: 35.4%\n",
      "Epoch: [4]  [ 0/18]  eta: 0:00:10  lr: 0.000470  min_lr: 0.000470  loss: 0.5139 (0.5139)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  time: 0.5798  data: 0.2615\n",
      "Epoch: [4]  [17/18]  eta: 0:00:00  lr: 0.000451  min_lr: 0.000451  loss: 0.5247 (0.5355)  class_acc: 0.8750 (0.8628)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.1224 (11.0988)  time: 0.5798  data: 0.2586\n",
      "Epoch: [4] Total time: 0:00:10 (0.5798 s / it)\n",
      "Averaged stats: lr: 0.000451  min_lr: 0.000451  loss: 0.5247 (0.5355)  class_acc: 0.8750 (0.8628)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.1224 (11.0988)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 1.3560 (1.3560)  acc1: 0.0000 (0.0000)  acc5: 100.0000 (100.0000)  time: 0.6932  data: 0.4429\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.9176 (0.8900)  acc1: 29.1667 (35.4331)  acc5: 100.0000 (100.0000)  time: 0.5853  data: 0.3716\n",
      "Test: Total time: 0:00:01 (0.5855 s / it)\n",
      "* Acc@1 35.433 Acc@5 100.000 loss 0.890\n",
      "Accuracy of the model on the 127 test images: 35.4%\n",
      "Epoch: [5]  [ 0/18]  eta: 0:00:10  lr: 0.000448  min_lr: 0.000448  loss: 0.5026 (0.5026)  class_acc: 0.9375 (0.9375)  weight_decay: 0.5000 (0.5000)  time: 0.6103  data: 0.2881\n",
      "Epoch: [5]  [17/18]  eta: 0:00:00  lr: 0.000424  min_lr: 0.000424  loss: 0.5026 (0.5302)  class_acc: 0.9062 (0.8837)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.6215 (12.7246)  time: 0.5870  data: 0.2655\n",
      "Epoch: [5] Total time: 0:00:10 (0.5871 s / it)\n",
      "Averaged stats: lr: 0.000424  min_lr: 0.000424  loss: 0.5026 (0.5302)  class_acc: 0.9062 (0.8837)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.6215 (12.7246)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.2601 (0.2601)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6628  data: 0.4162\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.2315 (0.2175)  acc1: 96.7742 (96.8504)  acc5: 100.0000 (100.0000)  time: 0.5623  data: 0.3510\n",
      "Test: Total time: 0:00:01 (0.5625 s / it)\n",
      "* Acc@1 96.850 Acc@5 100.000 loss 0.217\n",
      "Accuracy of the model on the 127 test images: 96.9%\n",
      "Epoch: [6]  [ 0/18]  eta: 0:00:09  lr: 0.000421  min_lr: 0.000421  loss: 0.5389 (0.5389)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  time: 0.5277  data: 0.2083\n",
      "Epoch: [6]  [17/18]  eta: 0:00:00  lr: 0.000393  min_lr: 0.000393  loss: 0.5049 (0.5173)  class_acc: 0.9062 (0.8854)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.1454 (12.3864)  time: 0.5743  data: 0.2526\n",
      "Epoch: [6] Total time: 0:00:10 (0.5743 s / it)\n",
      "Averaged stats: lr: 0.000393  min_lr: 0.000393  loss: 0.5049 (0.5173)  class_acc: 0.9062 (0.8854)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.1454 (12.3864)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.2707 (0.2707)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6790  data: 0.4253\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.2707 (0.2529)  acc1: 93.7500 (95.2756)  acc5: 100.0000 (100.0000)  time: 0.5780  data: 0.3639\n",
      "Test: Total time: 0:00:01 (0.5782 s / it)\n",
      "* Acc@1 95.276 Acc@5 100.000 loss 0.253\n",
      "Accuracy of the model on the 127 test images: 95.3%\n",
      "Epoch: [7]  [ 0/18]  eta: 0:00:10  lr: 0.000389  min_lr: 0.000389  loss: 0.5198 (0.5198)  class_acc: 0.8750 (0.8750)  weight_decay: 0.5000 (0.5000)  time: 0.5980  data: 0.2765\n",
      "Epoch: [7]  [17/18]  eta: 0:00:00  lr: 0.000358  min_lr: 0.000358  loss: 0.5234 (0.5364)  class_acc: 0.8750 (0.8819)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.4248 (11.7405)  time: 0.5778  data: 0.2552\n",
      "Epoch: [7] Total time: 0:00:10 (0.5778 s / it)\n",
      "Averaged stats: lr: 0.000358  min_lr: 0.000358  loss: 0.5234 (0.5364)  class_acc: 0.8750 (0.8819)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.4248 (11.7405)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.2265 (0.2265)  acc1: 95.8333 (95.8333)  acc5: 100.0000 (100.0000)  time: 0.6514  data: 0.4090\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1449 (0.1721)  acc1: 97.9167 (97.6378)  acc5: 100.0000 (100.0000)  time: 0.5672  data: 0.3534\n",
      "Test: Total time: 0:00:01 (0.5674 s / it)\n",
      "* Acc@1 97.638 Acc@5 100.000 loss 0.172\n",
      "Accuracy of the model on the 127 test images: 97.6%\n",
      "Epoch: [8]  [ 0/18]  eta: 0:00:10  lr: 0.000353  min_lr: 0.000353  loss: 0.4827 (0.4827)  class_acc: 0.8750 (0.8750)  weight_decay: 0.5000 (0.5000)  time: 0.6008  data: 0.2752\n",
      "Epoch: [8]  [17/18]  eta: 0:00:00  lr: 0.000319  min_lr: 0.000319  loss: 0.5078 (0.5033)  class_acc: 0.9062 (0.8958)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.0474 (9.9247)  time: 0.6005  data: 0.2747\n",
      "Epoch: [8] Total time: 0:00:10 (0.6005 s / it)\n",
      "Averaged stats: lr: 0.000319  min_lr: 0.000319  loss: 0.5078 (0.5033)  class_acc: 0.9062 (0.8958)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.0474 (9.9247)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.3012 (0.3012)  acc1: 85.4167 (85.4167)  acc5: 100.0000 (100.0000)  time: 0.7162  data: 0.4521\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.2649 (0.2079)  acc1: 96.7742 (93.7008)  acc5: 100.0000 (100.0000)  time: 0.5870  data: 0.3706\n",
      "Test: Total time: 0:00:01 (0.5872 s / it)\n",
      "* Acc@1 93.701 Acc@5 100.000 loss 0.208\n",
      "Accuracy of the model on the 127 test images: 93.7%\n",
      "Epoch: [9]  [ 0/18]  eta: 0:00:10  lr: 0.000315  min_lr: 0.000315  loss: 0.4965 (0.4965)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  time: 0.5968  data: 0.2781\n",
      "Epoch: [9]  [17/18]  eta: 0:00:00  lr: 0.000280  min_lr: 0.000280  loss: 0.4689 (0.4900)  class_acc: 0.9062 (0.8976)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.2031 (9.6730)  time: 0.5848  data: 0.2614\n",
      "Epoch: [9] Total time: 0:00:10 (0.5848 s / it)\n",
      "Averaged stats: lr: 0.000280  min_lr: 0.000280  loss: 0.4689 (0.4900)  class_acc: 0.9062 (0.8976)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.2031 (9.6730)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.1903 (0.1903)  acc1: 97.9167 (97.9167)  acc5: 100.0000 (100.0000)  time: 0.6587  data: 0.4142\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1903 (0.1970)  acc1: 97.9167 (97.6378)  acc5: 100.0000 (100.0000)  time: 0.5720  data: 0.3597\n",
      "Test: Total time: 0:00:01 (0.5722 s / it)\n",
      "* Acc@1 97.638 Acc@5 100.000 loss 0.197\n",
      "Accuracy of the model on the 127 test images: 97.6%\n",
      "Epoch: [10]  [ 0/18]  eta: 0:00:10  lr: 0.000275  min_lr: 0.000275  loss: 0.6539 (0.6539)  class_acc: 0.8438 (0.8438)  weight_decay: 0.5000 (0.5000)  time: 0.5772  data: 0.2549\n",
      "Epoch: [10]  [17/18]  eta: 0:00:00  lr: 0.000239  min_lr: 0.000239  loss: 0.4767 (0.5011)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.2127 (10.5403)  time: 0.5800  data: 0.2578\n",
      "Epoch: [10] Total time: 0:00:10 (0.5800 s / it)\n",
      "Averaged stats: lr: 0.000239  min_lr: 0.000239  loss: 0.4767 (0.5011)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  grad_norm: 11.2127 (10.5403)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.1772 (0.1772)  acc1: 93.7500 (93.7500)  acc5: 100.0000 (100.0000)  time: 0.6641  data: 0.4168\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1772 (0.1784)  acc1: 93.7500 (96.0630)  acc5: 100.0000 (100.0000)  time: 0.5705  data: 0.3584\n",
      "Test: Total time: 0:00:01 (0.5707 s / it)\n",
      "* Acc@1 96.063 Acc@5 100.000 loss 0.178\n",
      "Accuracy of the model on the 127 test images: 96.1%\n",
      "Epoch: [11]  [ 0/18]  eta: 0:00:10  lr: 0.000235  min_lr: 0.000235  loss: 0.4480 (0.4480)  class_acc: 0.9688 (0.9688)  weight_decay: 0.5000 (0.5000)  time: 0.5844  data: 0.2649\n",
      "Epoch: [11]  [17/18]  eta: 0:00:00  lr: 0.000199  min_lr: 0.000199  loss: 0.4804 (0.4952)  class_acc: 0.9062 (0.9080)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.3353 (10.0594)  time: 0.5822  data: 0.2591\n",
      "Epoch: [11] Total time: 0:00:10 (0.5822 s / it)\n",
      "Averaged stats: lr: 0.000199  min_lr: 0.000199  loss: 0.4804 (0.4952)  class_acc: 0.9062 (0.9080)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.3353 (10.0594)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.1281 (0.1281)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6605  data: 0.4138\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1281 (0.1258)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.5662  data: 0.3548\n",
      "Test: Total time: 0:00:01 (0.5664 s / it)\n",
      "* Acc@1 100.000 Acc@5 100.000 loss 0.126\n",
      "Accuracy of the model on the 127 test images: 100.0%\n",
      "Epoch: [12]  [ 0/18]  eta: 0:00:10  lr: 0.000195  min_lr: 0.000195  loss: 0.4434 (0.4434)  class_acc: 0.9688 (0.9688)  weight_decay: 0.5000 (0.5000)  time: 0.5639  data: 0.2438\n",
      "Epoch: [12]  [17/18]  eta: 0:00:00  lr: 0.000161  min_lr: 0.000161  loss: 0.4434 (0.4554)  class_acc: 0.9375 (0.9219)  weight_decay: 0.5000 (0.5000)  grad_norm: 7.9023 (8.0201)  time: 0.5840  data: 0.2614\n",
      "Epoch: [12] Total time: 0:00:10 (0.5841 s / it)\n",
      "Averaged stats: lr: 0.000161  min_lr: 0.000161  loss: 0.4434 (0.4554)  class_acc: 0.9375 (0.9219)  weight_decay: 0.5000 (0.5000)  grad_norm: 7.9023 (8.0201)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.1123 (0.1123)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6582  data: 0.4115\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1123 (0.1448)  acc1: 100.0000 (98.4252)  acc5: 100.0000 (100.0000)  time: 0.5672  data: 0.3554\n",
      "Test: Total time: 0:00:01 (0.5674 s / it)\n",
      "* Acc@1 98.425 Acc@5 100.000 loss 0.145\n",
      "Accuracy of the model on the 127 test images: 98.4%\n",
      "Epoch: [13]  [ 0/18]  eta: 0:00:10  lr: 0.000157  min_lr: 0.000157  loss: 0.4391 (0.4391)  class_acc: 0.9375 (0.9375)  weight_decay: 0.5000 (0.5000)  time: 0.6036  data: 0.2828\n",
      "Epoch: [13]  [17/18]  eta: 0:00:00  lr: 0.000125  min_lr: 0.000125  loss: 0.4391 (0.4591)  class_acc: 0.9375 (0.9236)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.7987 (9.4229)  time: 0.5875  data: 0.2643\n",
      "Epoch: [13] Total time: 0:00:10 (0.5875 s / it)\n",
      "Averaged stats: lr: 0.000125  min_lr: 0.000125  loss: 0.4391 (0.4591)  class_acc: 0.9375 (0.9236)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.7987 (9.4229)\n",
      "Test:  [0/3]  eta: 0:00:02  loss: 0.0780 (0.0780)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6873  data: 0.4390\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0944 (0.2062)  acc1: 100.0000 (96.8504)  acc5: 100.0000 (100.0000)  time: 0.5769  data: 0.3633\n",
      "Test: Total time: 0:00:01 (0.5770 s / it)\n",
      "* Acc@1 96.850 Acc@5 100.000 loss 0.206\n",
      "Accuracy of the model on the 127 test images: 96.9%\n",
      "Epoch: [14]  [ 0/18]  eta: 0:00:11  lr: 0.000121  min_lr: 0.000121  loss: 0.4709 (0.4709)  class_acc: 0.9688 (0.9688)  weight_decay: 0.5000 (0.5000)  time: 0.6121  data: 0.2940\n",
      "Epoch: [14]  [17/18]  eta: 0:00:00  lr: 0.000092  min_lr: 0.000092  loss: 0.4258 (0.4498)  class_acc: 0.9375 (0.9253)  weight_decay: 0.5000 (0.5000)  grad_norm: 8.1650 (8.9686)  time: 0.5835  data: 0.2622\n",
      "Epoch: [14] Total time: 0:00:10 (0.5836 s / it)\n",
      "Averaged stats: lr: 0.000092  min_lr: 0.000092  loss: 0.4258 (0.4498)  class_acc: 0.9375 (0.9253)  weight_decay: 0.5000 (0.5000)  grad_norm: 8.1650 (8.9686)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.0960 (0.0960)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6385  data: 0.4037\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0960 (0.1538)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5612  data: 0.3527\n",
      "Test: Total time: 0:00:01 (0.5613 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.154\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Epoch: [15]  [ 0/18]  eta: 0:00:10  lr: 0.000089  min_lr: 0.000089  loss: 0.4069 (0.4069)  class_acc: 0.9375 (0.9375)  weight_decay: 0.5000 (0.5000)  time: 0.5974  data: 0.2744\n",
      "Epoch: [15]  [17/18]  eta: 0:00:00  lr: 0.000064  min_lr: 0.000064  loss: 0.4314 (0.4411)  class_acc: 0.9375 (0.9306)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.4276 (9.8652)  time: 0.5779  data: 0.2568\n",
      "Epoch: [15] Total time: 0:00:10 (0.5780 s / it)\n",
      "Averaged stats: lr: 0.000064  min_lr: 0.000064  loss: 0.4314 (0.4411)  class_acc: 0.9375 (0.9306)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.4276 (9.8652)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.1039 (0.1039)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6585  data: 0.4166\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.1039 (0.1376)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5671  data: 0.3573\n",
      "Test: Total time: 0:00:01 (0.5673 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.138\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Epoch: [16]  [ 0/18]  eta: 0:00:10  lr: 0.000062  min_lr: 0.000062  loss: 0.3733 (0.3733)  class_acc: 1.0000 (1.0000)  weight_decay: 0.5000 (0.5000)  time: 0.5556  data: 0.2367\n",
      "Epoch: [16]  [17/18]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000042  loss: 0.3962 (0.4309)  class_acc: 0.9688 (0.9427)  weight_decay: 0.5000 (0.5000)  grad_norm: 8.4281 (8.7654)  time: 0.5819  data: 0.2616\n",
      "Epoch: [16] Total time: 0:00:10 (0.5819 s / it)\n",
      "Averaged stats: lr: 0.000042  min_lr: 0.000042  loss: 0.3962 (0.4309)  class_acc: 0.9688 (0.9427)  weight_decay: 0.5000 (0.5000)  grad_norm: 8.4281 (8.7654)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.0851 (0.0851)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6453  data: 0.4017\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0851 (0.1287)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5573  data: 0.3483\n",
      "Test: Total time: 0:00:01 (0.5574 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.129\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Epoch: [17]  [ 0/18]  eta: 0:00:10  lr: 0.000040  min_lr: 0.000040  loss: 0.4454 (0.4454)  class_acc: 0.9375 (0.9375)  weight_decay: 0.5000 (0.5000)  time: 0.5574  data: 0.2421\n",
      "Epoch: [17]  [17/18]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000025  loss: 0.4454 (0.4533)  class_acc: 0.9375 (0.9236)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.2091 (10.0456)  time: 0.5655  data: 0.2481\n",
      "Epoch: [17] Total time: 0:00:10 (0.5656 s / it)\n",
      "Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 0.4454 (0.4533)  class_acc: 0.9375 (0.9236)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.2091 (10.0456)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.0821 (0.0821)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6286  data: 0.3867\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0821 (0.1248)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5449  data: 0.3369\n",
      "Test: Total time: 0:00:01 (0.5450 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.125\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Epoch: [18]  [ 0/18]  eta: 0:00:10  lr: 0.000023  min_lr: 0.000023  loss: 0.4940 (0.4940)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  time: 0.5938  data: 0.2781\n",
      "Epoch: [18]  [17/18]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000014  loss: 0.4323 (0.4500)  class_acc: 0.9375 (0.9253)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.6928 (10.2236)  time: 0.5620  data: 0.2445\n",
      "Epoch: [18] Total time: 0:00:10 (0.5620 s / it)\n",
      "Averaged stats: lr: 0.000014  min_lr: 0.000014  loss: 0.4323 (0.4500)  class_acc: 0.9375 (0.9253)  weight_decay: 0.5000 (0.5000)  grad_norm: 10.6928 (10.2236)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.0731 (0.0731)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6461  data: 0.4047\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0731 (0.1217)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5542  data: 0.3463\n",
      "Test: Total time: 0:00:01 (0.5543 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.122\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Epoch: [19]  [ 0/18]  eta: 0:00:10  lr: 0.000013  min_lr: 0.000013  loss: 0.4272 (0.4272)  class_acc: 0.9062 (0.9062)  weight_decay: 0.5000 (0.5000)  time: 0.5691  data: 0.2533\n",
      "Epoch: [19]  [17/18]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000010  loss: 0.4292 (0.4445)  class_acc: 0.9375 (0.9323)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.1713 (9.6150)  time: 0.5620  data: 0.2444\n",
      "Epoch: [19] Total time: 0:00:10 (0.5620 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 0.4292 (0.4445)  class_acc: 0.9375 (0.9323)  weight_decay: 0.5000 (0.5000)  grad_norm: 9.1713 (9.6150)\n",
      "Test:  [0/3]  eta: 0:00:01  loss: 0.0784 (0.0784)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6489  data: 0.4064\n",
      "Test:  [2/3]  eta: 0:00:00  loss: 0.0784 (0.1229)  acc1: 100.0000 (99.2126)  acc5: 100.0000 (100.0000)  time: 0.5564  data: 0.3480\n",
      "Test: Total time: 0:00:01 (0.5565 s / it)\n",
      "* Acc@1 99.213 Acc@5 100.000 loss 0.123\n",
      "Accuracy of the model on the 127 test images: 99.2%\n",
      "Training time 0:04:07\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 32 --epochs 20 --model rcvit_xs --data_path data/meuDataset/ --nb_classes 3 --lr 5e-4 --min_lr 1e-5 --weight_decay 0.5 --output_dir data/res --finetune model/cas-vit-xs.pth --warmup_epochs 1 --data_set image_folder  --adapter True --device {device}   --model_ema_eval False --model_ema False --input_size 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste Dos Modelos no meu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 3\n",
      "Finetune resume checkpoint: data/res_fine/checkpoint-best_224.pth\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        42\n",
      "           1       1.00      0.93      0.96        40\n",
      "           2       0.90      0.96      0.93        46\n",
      "\n",
      "    accuracy                           0.94       128\n",
      "   macro avg       0.94      0.94      0.94       128\n",
      "weighted avg       0.94      0.94      0.94       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 32 --model rcvit_xs --data_path data/meuDataset/ --nb_classes 3 --finetune data/res_fine/checkpoint-best*.pth  --data_set image_folder  --adapter False --device {device}  --test True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 3\n",
      "Finetune resume checkpoint: data/res/checkpoint-best_384.pth\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        42\n",
      "           1       0.97      0.95      0.96        40\n",
      "           2       0.96      1.00      0.98        46\n",
      "\n",
      "    accuracy                           0.97       128\n",
      "   macro avg       0.97      0.97      0.97       128\n",
      "weighted avg       0.97      0.97      0.97       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 32 --model rcvit_xs --data_path data/meuDataset/ --nb_classes 3 --finetune data/res/checkpoint-best*.pth  --data_set image_folder  --adapter True --device {device}  --test True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUB TREINO E TESTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento com finetuning no Cas-Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 200\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x322597fd0>\n",
      "log writter dir:  None\n",
      "size mismatch for head.weight: copying a param with shape torch.Size([1000, 220]) from checkpoint, the shape in current model is torch.Size([200, 220]).\n",
      "size mismatch for head.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([200]).\n",
      "Finetune resume checkpoint: model/cas-vit-xs.pth\n",
      "number of params: 2802900\n",
      "LR = 0.01000000\n",
      "Batch size = 512\n",
      "Update frequent = 2\n",
      "Number of training examples = 5981\n",
      "Number of training training per epoch = 11\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Use Cosine LR scheduler\n",
      "Set warmup steps = 33\n",
      "epochs: 175, niter_per_ep: 11,  len(schedule): 1925\n",
      "Set warmup steps = 0\n",
      "epochs: 175, niter_per_ep: 11,  len(schedule): 1925\n",
      "Max WD = 0.5000000, Min WD = 0.5000000\n",
      "criterion = LabelSmoothingCrossEntropy()\n",
      "Auto resume checkpoint: data/IMres_fine/checkpoint-140.pth\n",
      "Resume checkpoint data/IMres_fine/checkpoint-140.pth\n",
      "With optim & sched!\n",
      "Total Trainable Params: 2802900\n",
      "Start training for 175 epochs\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 256 --epochs 100  --model rcvit_xs --data_path data/cub100/ --nb_classes 100 --lr 1e-4 --min_lr 5e-5 --weight_decay 0.5 --output_dir data/IMres_fine/ --finetune model/cas-vit-xs.pth --warmup_epochs 3 --data_set image_folder  --adapter False --device mps  --model_ema_eval False --model_ema False --disable_eval True --input_size 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento com Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 100\n",
      "Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x16bc48f10>\n",
      "log writter dir:  None\n",
      "Weights of RCViT_adapter not initialized from pretrained model: ['cur_adapter.0.adapter_layer_norm_before.weight', 'cur_adapter.0.adapter_layer_norm_before.bias', 'cur_adapter.0.adapter_layer_norm_before.running_mean', 'cur_adapter.0.adapter_layer_norm_before.running_var', 'cur_adapter.0.down_proj.weight', 'cur_adapter.0.down_proj.bias', 'cur_adapter.0.up_proj.weight', 'cur_adapter.0.up_proj.bias', 'cur_adapter.1.adapter_layer_norm_before.weight', 'cur_adapter.1.adapter_layer_norm_before.bias', 'cur_adapter.1.adapter_layer_norm_before.running_mean', 'cur_adapter.1.adapter_layer_norm_before.running_var', 'cur_adapter.1.down_proj.weight', 'cur_adapter.1.down_proj.bias', 'cur_adapter.1.up_proj.weight', 'cur_adapter.1.up_proj.bias', 'cur_adapter.2.adapter_layer_norm_before.weight', 'cur_adapter.2.adapter_layer_norm_before.bias', 'cur_adapter.2.adapter_layer_norm_before.running_mean', 'cur_adapter.2.adapter_layer_norm_before.running_var', 'cur_adapter.2.down_proj.weight', 'cur_adapter.2.down_proj.bias', 'cur_adapter.2.up_proj.weight', 'cur_adapter.2.up_proj.bias', 'cur_adapter.3.adapter_layer_norm_before.weight', 'cur_adapter.3.adapter_layer_norm_before.bias', 'cur_adapter.3.adapter_layer_norm_before.running_mean', 'cur_adapter.3.adapter_layer_norm_before.running_var', 'cur_adapter.3.down_proj.weight', 'cur_adapter.3.down_proj.bias', 'cur_adapter.3.up_proj.weight', 'cur_adapter.3.up_proj.bias', 'cur_adapter.4.adapter_layer_norm_before.weight', 'cur_adapter.4.adapter_layer_norm_before.bias', 'cur_adapter.4.adapter_layer_norm_before.running_mean', 'cur_adapter.4.adapter_layer_norm_before.running_var', 'cur_adapter.4.down_proj.weight', 'cur_adapter.4.down_proj.bias', 'cur_adapter.4.up_proj.weight', 'cur_adapter.4.up_proj.bias', 'cur_adapter.5.adapter_layer_norm_before.weight', 'cur_adapter.5.adapter_layer_norm_before.bias', 'cur_adapter.5.adapter_layer_norm_before.running_mean', 'cur_adapter.5.adapter_layer_norm_before.running_var', 'cur_adapter.5.down_proj.weight', 'cur_adapter.5.down_proj.bias', 'cur_adapter.5.up_proj.weight', 'cur_adapter.5.up_proj.bias', 'cur_adapter.6.adapter_layer_norm_before.weight', 'cur_adapter.6.adapter_layer_norm_before.bias', 'cur_adapter.6.adapter_layer_norm_before.running_mean', 'cur_adapter.6.adapter_layer_norm_before.running_var', 'cur_adapter.6.down_proj.weight', 'cur_adapter.6.down_proj.bias', 'cur_adapter.6.up_proj.weight', 'cur_adapter.6.up_proj.bias']\n",
      "size mismatch for head.weight: copying a param with shape torch.Size([1000, 220]) from checkpoint, the shape in current model is torch.Size([100, 220]).\n",
      "size mismatch for head.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([100]).\n",
      "Finetune resume checkpoint: model/cas-vit-xs.pth\n",
      "number of params: 155448\n",
      "Cas-Vit with Adapters ----------------------\n",
      "LR = 0.00010000\n",
      "Batch size = 512\n",
      "Update frequent = 2\n",
      "Number of training examples = 4693\n",
      "Number of training training per epoch = 9\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Use Cosine LR scheduler\n",
      "Set warmup steps = 27\n",
      "epochs: 100, niter_per_ep: 9,  len(schedule): 900\n",
      "Set warmup steps = 0\n",
      "epochs: 100, niter_per_ep: 9,  len(schedule): 900\n",
      "Max WD = 0.0500000, Min WD = 0.0500000\n",
      "criterion = LabelSmoothingCrossEntropy()\n",
      "Auto resume checkpoint: \n",
      "Total Trainable Params: 155448\n",
      "Start training for 100 epochs\n",
      "/Users/haranaka/.pyenv/versions/3.9.6/envs/RN/lib/python3.9/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch: [0]  [ 0/18]  eta: 0:00:45  lr: 0.000000  min_lr: 0.000000  loss: 4.8962 (4.8962)  class_acc: 0.0000 (0.0000)  weight_decay: 0.0500 (0.0500)  time: 2.5250  data: 0.8591\n",
      "Epoch: [0]  [17/18]  eta: 0:00:02  lr: 0.000031  min_lr: 0.000031  loss: 4.8018 (4.8041)  class_acc: 0.0078 (0.0104)  weight_decay: 0.0500 (0.0500)  grad_norm: 64.4720 (61.5292)  time: 2.1991  data: 0.9619\n",
      "Epoch: [0] Total time: 0:00:39 (2.1992 s / it)\n",
      "Averaged stats: lr: 0.000031  min_lr: 0.000031  loss: 4.8018 (4.8041)  class_acc: 0.0078 (0.0104)  weight_decay: 0.0500 (0.0500)  grad_norm: 64.4720 (61.5292)\n",
      "Epoch: [1]  [ 0/18]  eta: 0:00:35  lr: 0.000035  min_lr: 0.000035  loss: 4.7094 (4.7094)  class_acc: 0.0039 (0.0039)  weight_decay: 0.0500 (0.0500)  time: 1.9829  data: 0.8936\n",
      "Epoch: [1]  [17/18]  eta: 0:00:01  lr: 0.000065  min_lr: 0.000065  loss: 4.6332 (4.6486)  class_acc: 0.0117 (0.0139)  weight_decay: 0.0500 (0.0500)  grad_norm: 38.4081 (37.9891)  time: 1.9931  data: 0.9379\n",
      "Epoch: [1] Total time: 0:00:35 (1.9932 s / it)\n",
      "Averaged stats: lr: 0.000065  min_lr: 0.000065  loss: 4.6332 (4.6486)  class_acc: 0.0117 (0.0139)  weight_decay: 0.0500 (0.0500)  grad_norm: 38.4081 (37.9891)\n",
      "Epoch: [2]  [ 0/18]  eta: 0:00:43  lr: 0.000069  min_lr: 0.000069  loss: 4.6044 (4.6044)  class_acc: 0.0234 (0.0234)  weight_decay: 0.0500 (0.0500)  time: 2.4329  data: 0.8996\n",
      "Epoch: [2]  [17/18]  eta: 0:00:02  lr: 0.000100  min_lr: 0.000100  loss: 4.4958 (4.5140)  class_acc: 0.0312 (0.0323)  weight_decay: 0.0500 (0.0500)  grad_norm: 25.4932 (25.7729)  time: 2.0548  data: 0.9478\n",
      "Epoch: [2] Total time: 0:00:36 (2.0549 s / it)\n",
      "Averaged stats: lr: 0.000100  min_lr: 0.000100  loss: 4.4958 (4.5140)  class_acc: 0.0312 (0.0323)  weight_decay: 0.0500 (0.0500)  grad_norm: 25.4932 (25.7729)\n",
      "Epoch: [3]  [ 0/18]  eta: 0:00:37  lr: 0.000100  min_lr: 0.000100  loss: 4.4736 (4.4736)  class_acc: 0.0156 (0.0156)  weight_decay: 0.0500 (0.0500)  time: 2.0956  data: 0.9792\n",
      "Epoch: [3]  [17/18]  eta: 0:00:02  lr: 0.000100  min_lr: 0.000100  loss: 4.3521 (4.3599)  class_acc: 0.0391 (0.0393)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0409 (19.6420)  time: 2.0281  data: 0.9621\n",
      "Epoch: [3] Total time: 0:00:36 (2.0282 s / it)\n",
      "Averaged stats: lr: 0.000100  min_lr: 0.000100  loss: 4.3521 (4.3599)  class_acc: 0.0391 (0.0393)  weight_decay: 0.0500 (0.0500)  grad_norm: 20.0409 (19.6420)\n",
      "Epoch: [4]  [ 0/18]  eta: 0:00:36  lr: 0.000100  min_lr: 0.000100  loss: 4.3697 (4.3697)  class_acc: 0.0469 (0.0469)  weight_decay: 0.0500 (0.0500)  time: 2.0478  data: 0.9042\n",
      "Epoch: [4]  [17/18]  eta: 0:00:01  lr: 0.000100  min_lr: 0.000100  loss: 4.2357 (4.2429)  class_acc: 0.0547 (0.0525)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1251 (17.1271)  time: 1.9862  data: 0.9160\n",
      "Epoch: [4] Total time: 0:00:35 (1.9863 s / it)\n",
      "Averaged stats: lr: 0.000100  min_lr: 0.000100  loss: 4.2357 (4.2429)  class_acc: 0.0547 (0.0525)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.1251 (17.1271)\n",
      "Epoch: [5]  [ 0/18]  eta: 0:00:38  lr: 0.000100  min_lr: 0.000100  loss: 4.2516 (4.2516)  class_acc: 0.0586 (0.0586)  weight_decay: 0.0500 (0.0500)  time: 2.1474  data: 0.9093\n",
      "Epoch: [5]  [17/18]  eta: 0:00:02  lr: 0.000100  min_lr: 0.000100  loss: 4.1492 (4.1554)  class_acc: 0.0664 (0.0673)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.2356 (18.1062)  time: 2.2320  data: 0.9547\n",
      "Epoch: [5] Total time: 0:00:40 (2.2321 s / it)\n",
      "Averaged stats: lr: 0.000100  min_lr: 0.000100  loss: 4.1492 (4.1554)  class_acc: 0.0664 (0.0673)  weight_decay: 0.0500 (0.0500)  grad_norm: 17.2356 (18.1062)\n",
      "Epoch: [6]  [ 0/18]  eta: 0:00:38  lr: 0.000100  min_lr: 0.000100  loss: 4.1605 (4.1605)  class_acc: 0.0742 (0.0742)  weight_decay: 0.0500 (0.0500)  time: 2.1239  data: 0.9712\n",
      "Epoch: [6]  [17/18]  eta: 0:00:01  lr: 0.000100  min_lr: 0.000100  loss: 4.0891 (4.0804)  class_acc: 0.0820 (0.0849)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8656 (19.7515)  time: 1.9817  data: 0.9415\n",
      "Epoch: [6] Total time: 0:00:35 (1.9818 s / it)\n",
      "Averaged stats: lr: 0.000100  min_lr: 0.000100  loss: 4.0891 (4.0804)  class_acc: 0.0820 (0.0849)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8656 (19.7515)\n",
      "Epoch: [7]  [ 0/18]  eta: 0:00:39  lr: 0.000100  min_lr: 0.000100  loss: 4.0863 (4.0863)  class_acc: 0.0781 (0.0781)  weight_decay: 0.0500 (0.0500)  time: 2.1773  data: 0.9550\n",
      "Epoch: [7]  [17/18]  eta: 0:00:02  lr: 0.000099  min_lr: 0.000099  loss: 4.0123 (4.0307)  class_acc: 0.0977 (0.0955)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8240 (20.5373)  time: 2.0485  data: 0.9420\n",
      "Epoch: [7] Total time: 0:00:36 (2.0486 s / it)\n",
      "Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 4.0123 (4.0307)  class_acc: 0.0977 (0.0955)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8240 (20.5373)\n",
      "Epoch: [8]  [ 0/18]  eta: 0:00:37  lr: 0.000099  min_lr: 0.000099  loss: 3.9984 (3.9984)  class_acc: 0.0859 (0.0859)  weight_decay: 0.0500 (0.0500)  time: 2.0788  data: 0.9409\n",
      "Epoch: [8]  [17/18]  eta: 0:00:02  lr: 0.000099  min_lr: 0.000099  loss: 3.9593 (3.9601)  class_acc: 0.1133 (0.1113)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8106 (19.5562)  time: 2.1170  data: 0.9762\n",
      "Epoch: [8] Total time: 0:00:38 (2.1172 s / it)\n",
      "Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 3.9593 (3.9601)  class_acc: 0.1133 (0.1113)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.8106 (19.5562)\n",
      "Epoch: [9]  [ 0/18]  eta: 0:00:41  lr: 0.000099  min_lr: 0.000099  loss: 3.9225 (3.9225)  class_acc: 0.1133 (0.1133)  weight_decay: 0.0500 (0.0500)  time: 2.3133  data: 0.9011\n",
      "Epoch: [9]  [17/18]  eta: 0:00:02  lr: 0.000099  min_lr: 0.000099  loss: 3.8874 (3.8955)  class_acc: 0.1250 (0.1263)  weight_decay: 0.0500 (0.0500)  grad_norm: 21.4176 (21.3723)  time: 2.0529  data: 0.9246\n",
      "Epoch: [9] Total time: 0:00:36 (2.0530 s / it)\n",
      "Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 3.8874 (3.8955)  class_acc: 0.1250 (0.1263)  weight_decay: 0.0500 (0.0500)  grad_norm: 21.4176 (21.3723)\n",
      "Epoch: [10]  [ 0/18]  eta: 0:00:42  lr: 0.000099  min_lr: 0.000099  loss: 3.9271 (3.9271)  class_acc: 0.1406 (0.1406)  weight_decay: 0.0500 (0.0500)  time: 2.3711  data: 0.9121\n",
      "Epoch: [10]  [17/18]  eta: 0:00:02  lr: 0.000099  min_lr: 0.000099  loss: 3.8740 (3.8631)  class_acc: 0.1406 (0.1372)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.5929 (23.2826)  time: 2.2027  data: 0.9510\n",
      "Epoch: [10] Total time: 0:00:39 (2.2028 s / it)\n",
      "Averaged stats: lr: 0.000099  min_lr: 0.000099  loss: 3.8740 (3.8631)  class_acc: 0.1406 (0.1372)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.5929 (23.2826)\n",
      "Epoch: [11]  [ 0/18]  eta: 0:00:40  lr: 0.000098  min_lr: 0.000098  loss: 3.8546 (3.8546)  class_acc: 0.1289 (0.1289)  weight_decay: 0.0500 (0.0500)  time: 2.2680  data: 0.9241\n",
      "Epoch: [11]  [17/18]  eta: 0:00:02  lr: 0.000098  min_lr: 0.000098  loss: 3.8042 (3.8118)  class_acc: 0.1484 (0.1458)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.8117 (22.5046)  time: 2.1005  data: 0.9704\n",
      "Epoch: [11] Total time: 0:00:37 (2.1006 s / it)\n",
      "Averaged stats: lr: 0.000098  min_lr: 0.000098  loss: 3.8042 (3.8118)  class_acc: 0.1484 (0.1458)  weight_decay: 0.0500 (0.0500)  grad_norm: 22.8117 (22.5046)\n",
      "Epoch: [12]  [ 0/18]  eta: 0:00:44  lr: 0.000098  min_lr: 0.000098  loss: 3.8482 (3.8482)  class_acc: 0.1250 (0.1250)  weight_decay: 0.0500 (0.0500)  time: 2.4775  data: 1.0475\n",
      "Epoch: [12]  [17/18]  eta: 0:00:02  lr: 0.000098  min_lr: 0.000098  loss: 3.7704 (3.7757)  class_acc: 0.1484 (0.1510)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.8661 (23.7909)  time: 2.1532  data: 0.9533\n",
      "Epoch: [12] Total time: 0:00:38 (2.1534 s / it)\n",
      "Averaged stats: lr: 0.000098  min_lr: 0.000098  loss: 3.7704 (3.7757)  class_acc: 0.1484 (0.1510)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.8661 (23.7909)\n",
      "Epoch: [13]  [ 0/18]  eta: 0:00:40  lr: 0.000098  min_lr: 0.000098  loss: 3.7165 (3.7165)  class_acc: 0.1523 (0.1523)  weight_decay: 0.0500 (0.0500)  time: 2.2619  data: 0.9036\n",
      "Epoch: [13]  [17/18]  eta: 0:00:02  lr: 0.000097  min_lr: 0.000097  loss: 3.7398 (3.7397)  class_acc: 0.1523 (0.1582)  weight_decay: 0.0500 (0.0500)  grad_norm: 25.0921 (24.9236)  time: 2.2329  data: 0.9858\n",
      "Epoch: [13] Total time: 0:00:40 (2.2333 s / it)\n",
      "Averaged stats: lr: 0.000097  min_lr: 0.000097  loss: 3.7398 (3.7397)  class_acc: 0.1523 (0.1582)  weight_decay: 0.0500 (0.0500)  grad_norm: 25.0921 (24.9236)\n",
      "Epoch: [14]  [ 0/18]  eta: 0:00:41  lr: 0.000097  min_lr: 0.000097  loss: 3.7208 (3.7208)  class_acc: 0.1797 (0.1797)  weight_decay: 0.0500 (0.0500)  time: 2.3260  data: 0.9795\n",
      "Epoch: [14]  [17/18]  eta: 0:00:02  lr: 0.000097  min_lr: 0.000097  loss: 3.6779 (3.6858)  class_acc: 0.1758 (0.1771)  weight_decay: 0.0500 (0.0500)  grad_norm: 24.4460 (24.6972)  time: 2.1129  data: 0.9679\n",
      "Epoch: [14] Total time: 0:00:38 (2.1130 s / it)\n",
      "Averaged stats: lr: 0.000097  min_lr: 0.000097  loss: 3.6779 (3.6858)  class_acc: 0.1758 (0.1771)  weight_decay: 0.0500 (0.0500)  grad_norm: 24.4460 (24.6972)\n",
      "Epoch: [15]  [ 0/18]  eta: 0:00:40  lr: 0.000097  min_lr: 0.000097  loss: 3.6505 (3.6505)  class_acc: 0.1719 (0.1719)  weight_decay: 0.0500 (0.0500)  time: 2.2238  data: 0.9026\n",
      "Epoch: [15]  [17/18]  eta: 0:00:02  lr: 0.000096  min_lr: 0.000096  loss: 3.6572 (3.6646)  class_acc: 0.1836 (0.1862)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.8157 (26.8043)  time: 2.0893  data: 0.9399\n",
      "Epoch: [15] Total time: 0:00:37 (2.0894 s / it)\n",
      "Averaged stats: lr: 0.000096  min_lr: 0.000096  loss: 3.6572 (3.6646)  class_acc: 0.1836 (0.1862)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.8157 (26.8043)\n",
      "Epoch: [16]  [ 0/18]  eta: 0:00:41  lr: 0.000096  min_lr: 0.000096  loss: 3.6249 (3.6249)  class_acc: 0.1953 (0.1953)  weight_decay: 0.0500 (0.0500)  time: 2.3107  data: 0.9501\n",
      "Epoch: [16]  [17/18]  eta: 0:00:02  lr: 0.000096  min_lr: 0.000096  loss: 3.6353 (3.6479)  class_acc: 0.1797 (0.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.2204 (26.3728)  time: 2.2561  data: 0.9597\n",
      "Epoch: [16] Total time: 0:00:40 (2.2563 s / it)\n",
      "Averaged stats: lr: 0.000096  min_lr: 0.000096  loss: 3.6353 (3.6479)  class_acc: 0.1797 (0.1782)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.2204 (26.3728)\n",
      "Epoch: [17]  [ 0/18]  eta: 0:00:39  lr: 0.000095  min_lr: 0.000095  loss: 3.5871 (3.5871)  class_acc: 0.2070 (0.2070)  weight_decay: 0.0500 (0.0500)  time: 2.2043  data: 0.9199\n",
      "Epoch: [17]  [17/18]  eta: 0:00:02  lr: 0.000095  min_lr: 0.000095  loss: 3.6067 (3.6112)  class_acc: 0.1914 (0.1953)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.7166 (28.0678)  time: 2.0609  data: 0.9467\n",
      "Epoch: [17] Total time: 0:00:37 (2.0609 s / it)\n",
      "Averaged stats: lr: 0.000095  min_lr: 0.000095  loss: 3.6067 (3.6112)  class_acc: 0.1914 (0.1953)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.7166 (28.0678)\n",
      "Epoch: [18]  [ 0/18]  eta: 0:00:40  lr: 0.000095  min_lr: 0.000095  loss: 3.5179 (3.5179)  class_acc: 0.2031 (0.2031)  weight_decay: 0.0500 (0.0500)  time: 2.2635  data: 0.9704\n",
      "Epoch: [18]  [17/18]  eta: 0:00:02  lr: 0.000094  min_lr: 0.000094  loss: 3.6021 (3.5746)  class_acc: 0.2070 (0.2070)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.7143 (26.8156)  time: 2.0684  data: 0.9660\n",
      "Epoch: [18] Total time: 0:00:37 (2.0685 s / it)\n",
      "Averaged stats: lr: 0.000094  min_lr: 0.000094  loss: 3.6021 (3.5746)  class_acc: 0.2070 (0.2070)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.7143 (26.8156)\n",
      "Epoch: [19]  [ 0/18]  eta: 0:00:35  lr: 0.000094  min_lr: 0.000094  loss: 3.5686 (3.5686)  class_acc: 0.1758 (0.1758)  weight_decay: 0.0500 (0.0500)  time: 1.9612  data: 0.8885\n",
      "Epoch: [19]  [17/18]  eta: 0:00:02  lr: 0.000093  min_lr: 0.000093  loss: 3.5597 (3.5607)  class_acc: 0.2070 (0.2101)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.0679 (27.7844)  time: 2.0322  data: 0.9552\n",
      "Epoch: [19] Total time: 0:00:36 (2.0322 s / it)\n",
      "Averaged stats: lr: 0.000093  min_lr: 0.000093  loss: 3.5597 (3.5607)  class_acc: 0.2070 (0.2101)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.0679 (27.7844)\n",
      "Epoch: [20]  [ 0/18]  eta: 0:00:39  lr: 0.000093  min_lr: 0.000093  loss: 3.5504 (3.5504)  class_acc: 0.2383 (0.2383)  weight_decay: 0.0500 (0.0500)  time: 2.1855  data: 0.9094\n",
      "Epoch: [20]  [17/18]  eta: 0:00:01  lr: 0.000093  min_lr: 0.000093  loss: 3.5281 (3.5277)  class_acc: 0.2188 (0.2196)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.5069 (27.4807)  time: 1.9643  data: 0.9363\n",
      "Epoch: [20] Total time: 0:00:35 (1.9645 s / it)\n",
      "Averaged stats: lr: 0.000093  min_lr: 0.000093  loss: 3.5281 (3.5277)  class_acc: 0.2188 (0.2196)  weight_decay: 0.0500 (0.0500)  grad_norm: 26.5069 (27.4807)\n",
      "Epoch: [21]  [ 0/18]  eta: 0:00:40  lr: 0.000093  min_lr: 0.000093  loss: 3.5148 (3.5148)  class_acc: 0.2188 (0.2188)  weight_decay: 0.0500 (0.0500)  time: 2.2404  data: 0.8759\n",
      "Epoch: [21]  [17/18]  eta: 0:00:02  lr: 0.000092  min_lr: 0.000092  loss: 3.5413 (3.5315)  class_acc: 0.2266 (0.2207)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.6837 (28.4868)  time: 2.0556  data: 0.9400\n",
      "Epoch: [21] Total time: 0:00:37 (2.0557 s / it)\n",
      "Averaged stats: lr: 0.000092  min_lr: 0.000092  loss: 3.5413 (3.5315)  class_acc: 0.2266 (0.2207)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.6837 (28.4868)\n",
      "Epoch: [22]  [ 0/18]  eta: 0:00:38  lr: 0.000092  min_lr: 0.000092  loss: 3.4751 (3.4751)  class_acc: 0.2031 (0.2031)  weight_decay: 0.0500 (0.0500)  time: 2.1399  data: 0.9505\n",
      "Epoch: [22]  [17/18]  eta: 0:00:02  lr: 0.000091  min_lr: 0.000091  loss: 3.4713 (3.4744)  class_acc: 0.2188 (0.2259)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8247 (27.8421)  time: 2.0460  data: 0.9707\n",
      "Epoch: [22] Total time: 0:00:36 (2.0462 s / it)\n",
      "Averaged stats: lr: 0.000091  min_lr: 0.000091  loss: 3.4713 (3.4744)  class_acc: 0.2188 (0.2259)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8247 (27.8421)\n",
      "Epoch: [23]  [ 0/18]  eta: 0:00:40  lr: 0.000091  min_lr: 0.000091  loss: 3.4408 (3.4408)  class_acc: 0.2266 (0.2266)  weight_decay: 0.0500 (0.0500)  time: 2.2416  data: 0.9192\n",
      "Epoch: [23]  [17/18]  eta: 0:00:02  lr: 0.000090  min_lr: 0.000090  loss: 3.4729 (3.4848)  class_acc: 0.2227 (0.2250)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8202 (28.0857)  time: 2.0565  data: 0.9450\n",
      "Epoch: [23] Total time: 0:00:37 (2.0568 s / it)\n",
      "Averaged stats: lr: 0.000090  min_lr: 0.000090  loss: 3.4729 (3.4848)  class_acc: 0.2227 (0.2250)  weight_decay: 0.0500 (0.0500)  grad_norm: 27.8202 (28.0857)\n",
      "Epoch: [24]  [ 0/18]  eta: 0:00:38  lr: 0.000090  min_lr: 0.000090  loss: 3.4900 (3.4900)  class_acc: 0.2188 (0.2188)  weight_decay: 0.0500 (0.0500)  time: 2.1187  data: 0.9028\n",
      "Epoch: [24]  [17/18]  eta: 0:00:02  lr: 0.000089  min_lr: 0.000089  loss: 3.4254 (3.4325)  class_acc: 0.2344 (0.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.3339 (28.6469)  time: 2.0501  data: 0.9367\n",
      "Epoch: [24] Total time: 0:00:36 (2.0502 s / it)\n",
      "Averaged stats: lr: 0.000089  min_lr: 0.000089  loss: 3.4254 (3.4325)  class_acc: 0.2344 (0.2411)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.3339 (28.6469)\n",
      "Epoch: [25]  [ 0/18]  eta: 0:00:32  lr: 0.000089  min_lr: 0.000089  loss: 3.4477 (3.4477)  class_acc: 0.2266 (0.2266)  weight_decay: 0.0500 (0.0500)  time: 1.8262  data: 0.8647\n",
      "Epoch: [25]  [17/18]  eta: 0:00:02  lr: 0.000088  min_lr: 0.000088  loss: 3.4195 (3.4348)  class_acc: 0.2383 (0.2342)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.9321 (29.0170)  time: 2.2605  data: 0.9536\n",
      "Epoch: [25] Total time: 0:00:40 (2.2606 s / it)\n",
      "Averaged stats: lr: 0.000088  min_lr: 0.000088  loss: 3.4195 (3.4348)  class_acc: 0.2383 (0.2342)  weight_decay: 0.0500 (0.0500)  grad_norm: 28.9321 (29.0170)\n",
      "Epoch: [26]  [ 0/18]  eta: 0:00:54  lr: 0.000088  min_lr: 0.000088  loss: 3.4455 (3.4455)  class_acc: 0.2266 (0.2266)  weight_decay: 0.0500 (0.0500)  time: 3.0530  data: 0.9498\n",
      "Epoch: [26]  [17/18]  eta: 0:00:02  lr: 0.000087  min_lr: 0.000087  loss: 3.4242 (3.4283)  class_acc: 0.2383 (0.2389)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3689 (29.4546)  time: 2.2110  data: 0.9846\n",
      "Epoch: [26] Total time: 0:00:39 (2.2111 s / it)\n",
      "Averaged stats: lr: 0.000087  min_lr: 0.000087  loss: 3.4242 (3.4283)  class_acc: 0.2383 (0.2389)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.3689 (29.4546)\n",
      "Epoch: [27]  [ 0/18]  eta: 0:00:41  lr: 0.000087  min_lr: 0.000087  loss: 3.3923 (3.3923)  class_acc: 0.2773 (0.2773)  weight_decay: 0.0500 (0.0500)  time: 2.3114  data: 0.9508\n",
      "Epoch: [27]  [17/18]  eta: 0:00:02  lr: 0.000086  min_lr: 0.000086  loss: 3.3813 (3.3838)  class_acc: 0.2461 (0.2543)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.8415 (29.8367)  time: 2.0517  data: 0.9593\n",
      "Epoch: [27] Total time: 0:00:36 (2.0518 s / it)\n",
      "Averaged stats: lr: 0.000086  min_lr: 0.000086  loss: 3.3813 (3.3838)  class_acc: 0.2461 (0.2543)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.8415 (29.8367)\n",
      "Epoch: [28]  [ 0/18]  eta: 0:00:36  lr: 0.000086  min_lr: 0.000086  loss: 3.3908 (3.3908)  class_acc: 0.2305 (0.2305)  weight_decay: 0.0500 (0.0500)  time: 2.0542  data: 0.9450\n",
      "Epoch: [28]  [17/18]  eta: 0:00:01  lr: 0.000085  min_lr: 0.000085  loss: 3.3607 (3.3752)  class_acc: 0.2578 (0.2561)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.1102 (29.8384)  time: 1.9456  data: 0.9339\n",
      "Epoch: [28] Total time: 0:00:35 (1.9457 s / it)\n",
      "Averaged stats: lr: 0.000085  min_lr: 0.000085  loss: 3.3607 (3.3752)  class_acc: 0.2578 (0.2561)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.1102 (29.8384)\n",
      "Epoch: [29]  [ 0/18]  eta: 0:00:44  lr: 0.000085  min_lr: 0.000085  loss: 3.3430 (3.3430)  class_acc: 0.2695 (0.2695)  weight_decay: 0.0500 (0.0500)  time: 2.4642  data: 0.9377\n",
      "Epoch: [29]  [17/18]  eta: 0:00:02  lr: 0.000084  min_lr: 0.000084  loss: 3.3479 (3.3585)  class_acc: 0.2617 (0.2600)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.4999 (29.4252)  time: 2.0733  data: 0.9373\n",
      "Epoch: [29] Total time: 0:00:37 (2.0734 s / it)\n",
      "Averaged stats: lr: 0.000084  min_lr: 0.000084  loss: 3.3479 (3.3585)  class_acc: 0.2617 (0.2600)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.4999 (29.4252)\n",
      "Epoch: [30]  [ 0/18]  eta: 0:00:43  lr: 0.000084  min_lr: 0.000084  loss: 3.3730 (3.3730)  class_acc: 0.2422 (0.2422)  weight_decay: 0.0500 (0.0500)  time: 2.4142  data: 0.9383\n",
      "Epoch: [30]  [17/18]  eta: 0:00:02  lr: 0.000083  min_lr: 0.000083  loss: 3.3700 (3.3607)  class_acc: 0.2461 (0.2537)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.8882 (30.4424)  time: 2.0575  data: 0.9497\n",
      "Epoch: [30] Total time: 0:00:37 (2.0577 s / it)\n",
      "Averaged stats: lr: 0.000083  min_lr: 0.000083  loss: 3.3700 (3.3607)  class_acc: 0.2461 (0.2537)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.8882 (30.4424)\n",
      "Epoch: [31]  [ 0/18]  eta: 0:00:38  lr: 0.000083  min_lr: 0.000083  loss: 3.3566 (3.3566)  class_acc: 0.2461 (0.2461)  weight_decay: 0.0500 (0.0500)  time: 2.1300  data: 0.9296\n",
      "Epoch: [31]  [17/18]  eta: 0:00:01  lr: 0.000082  min_lr: 0.000082  loss: 3.3454 (3.3427)  class_acc: 0.2539 (0.2598)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.0142 (29.4900)  time: 1.9715  data: 0.9428\n",
      "Epoch: [31] Total time: 0:00:35 (1.9716 s / it)\n",
      "Averaged stats: lr: 0.000082  min_lr: 0.000082  loss: 3.3454 (3.3427)  class_acc: 0.2539 (0.2598)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.0142 (29.4900)\n",
      "Epoch: [32]  [ 0/18]  eta: 0:00:42  lr: 0.000082  min_lr: 0.000082  loss: 3.2717 (3.2717)  class_acc: 0.2852 (0.2852)  weight_decay: 0.0500 (0.0500)  time: 2.3802  data: 0.8805\n",
      "Epoch: [32]  [17/18]  eta: 0:00:02  lr: 0.000081  min_lr: 0.000081  loss: 3.2869 (3.3029)  class_acc: 0.2695 (0.2745)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.9550 (29.5092)  time: 2.0508  data: 0.9706\n",
      "Epoch: [32] Total time: 0:00:36 (2.0509 s / it)\n",
      "Averaged stats: lr: 0.000081  min_lr: 0.000081  loss: 3.2869 (3.3029)  class_acc: 0.2695 (0.2745)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.9550 (29.5092)\n",
      "Epoch: [33]  [ 0/18]  eta: 0:00:39  lr: 0.000080  min_lr: 0.000080  loss: 3.3548 (3.3548)  class_acc: 0.2422 (0.2422)  weight_decay: 0.0500 (0.0500)  time: 2.2017  data: 0.9444\n",
      "Epoch: [33]  [17/18]  eta: 0:00:02  lr: 0.000079  min_lr: 0.000079  loss: 3.3144 (3.3246)  class_acc: 0.2656 (0.2669)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.4228 (30.3527)  time: 2.1168  data: 0.9415\n",
      "Epoch: [33] Total time: 0:00:38 (2.1169 s / it)\n",
      "Averaged stats: lr: 0.000079  min_lr: 0.000079  loss: 3.3144 (3.3246)  class_acc: 0.2656 (0.2669)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.4228 (30.3527)\n",
      "Epoch: [34]  [ 0/18]  eta: 0:00:43  lr: 0.000079  min_lr: 0.000079  loss: 3.2279 (3.2279)  class_acc: 0.2656 (0.2656)  weight_decay: 0.0500 (0.0500)  time: 2.4053  data: 0.9811\n",
      "Epoch: [34]  [17/18]  eta: 0:00:02  lr: 0.000078  min_lr: 0.000078  loss: 3.2841 (3.2972)  class_acc: 0.2656 (0.2667)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8257 (31.1763)  time: 2.0932  data: 0.9804\n",
      "Epoch: [34] Total time: 0:00:37 (2.0932 s / it)\n",
      "Averaged stats: lr: 0.000078  min_lr: 0.000078  loss: 3.2841 (3.2972)  class_acc: 0.2656 (0.2667)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8257 (31.1763)\n",
      "Epoch: [35]  [ 0/18]  eta: 0:00:40  lr: 0.000078  min_lr: 0.000078  loss: 3.1303 (3.1303)  class_acc: 0.3125 (0.3125)  weight_decay: 0.0500 (0.0500)  time: 2.2712  data: 0.9060\n",
      "Epoch: [35]  [17/18]  eta: 0:00:02  lr: 0.000077  min_lr: 0.000077  loss: 3.2602 (3.2712)  class_acc: 0.2734 (0.2823)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0653 (30.7529)  time: 2.0361  data: 0.9593\n",
      "Epoch: [35] Total time: 0:00:36 (2.0362 s / it)\n",
      "Averaged stats: lr: 0.000077  min_lr: 0.000077  loss: 3.2602 (3.2712)  class_acc: 0.2734 (0.2823)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0653 (30.7529)\n",
      "Epoch: [36]  [ 0/18]  eta: 0:00:35  lr: 0.000077  min_lr: 0.000077  loss: 3.2500 (3.2500)  class_acc: 0.3203 (0.3203)  weight_decay: 0.0500 (0.0500)  time: 1.9791  data: 0.8865\n",
      "Epoch: [36]  [17/18]  eta: 0:00:02  lr: 0.000076  min_lr: 0.000076  loss: 3.2681 (3.2657)  class_acc: 0.2773 (0.2828)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0464 (31.1039)  time: 2.0617  data: 0.9682\n",
      "Epoch: [36] Total time: 0:00:37 (2.0620 s / it)\n",
      "Averaged stats: lr: 0.000076  min_lr: 0.000076  loss: 3.2681 (3.2657)  class_acc: 0.2773 (0.2828)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0464 (31.1039)\n",
      "Epoch: [37]  [ 0/18]  eta: 0:00:42  lr: 0.000075  min_lr: 0.000075  loss: 3.2258 (3.2258)  class_acc: 0.3164 (0.3164)  weight_decay: 0.0500 (0.0500)  time: 2.3477  data: 0.9098\n",
      "Epoch: [37]  [17/18]  eta: 0:00:02  lr: 0.000074  min_lr: 0.000074  loss: 3.2531 (3.2539)  class_acc: 0.2852 (0.2888)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8477 (30.8539)  time: 2.0756  data: 0.9425\n",
      "Epoch: [37] Total time: 0:00:37 (2.0757 s / it)\n",
      "Averaged stats: lr: 0.000074  min_lr: 0.000074  loss: 3.2531 (3.2539)  class_acc: 0.2852 (0.2888)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8477 (30.8539)\n",
      "Epoch: [38]  [ 0/18]  eta: 0:00:42  lr: 0.000074  min_lr: 0.000074  loss: 3.2492 (3.2492)  class_acc: 0.2812 (0.2812)  weight_decay: 0.0500 (0.0500)  time: 2.3560  data: 0.8815\n",
      "Epoch: [38]  [17/18]  eta: 0:00:04  lr: 0.000073  min_lr: 0.000073  loss: 3.2365 (3.2441)  class_acc: 0.2812 (0.2847)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.4449 (30.5131)  time: 4.1007  data: 2.6030\n",
      "Epoch: [38] Total time: 0:01:13 (4.1011 s / it)\n",
      "Averaged stats: lr: 0.000073  min_lr: 0.000073  loss: 3.2365 (3.2441)  class_acc: 0.2812 (0.2847)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.4449 (30.5131)\n",
      "Epoch: [39]  [ 0/18]  eta: 0:00:40  lr: 0.000073  min_lr: 0.000073  loss: 3.2271 (3.2271)  class_acc: 0.2578 (0.2578)  weight_decay: 0.0500 (0.0500)  time: 2.2444  data: 0.8911\n",
      "Epoch: [39]  [17/18]  eta: 0:00:14  lr: 0.000072  min_lr: 0.000072  loss: 3.2560 (3.2521)  class_acc: 0.2734 (0.2841)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2757 (31.0916)  time: 14.2625  data: 0.9285\n",
      "Epoch: [39] Total time: 0:04:16 (14.2629 s / it)\n",
      "Averaged stats: lr: 0.000072  min_lr: 0.000072  loss: 3.2560 (3.2521)  class_acc: 0.2734 (0.2841)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2757 (31.0916)\n",
      "Epoch: [40]  [ 0/18]  eta: 0:00:42  lr: 0.000071  min_lr: 0.000071  loss: 3.2217 (3.2217)  class_acc: 0.2695 (0.2695)  weight_decay: 0.0500 (0.0500)  time: 2.3766  data: 0.9561\n",
      "Epoch: [40]  [17/18]  eta: 0:00:02  lr: 0.000070  min_lr: 0.000070  loss: 3.2294 (3.2300)  class_acc: 0.2891 (0.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.7365 (30.9186)  time: 2.0594  data: 0.9535\n",
      "Epoch: [40] Total time: 0:00:37 (2.0596 s / it)\n",
      "Averaged stats: lr: 0.000070  min_lr: 0.000070  loss: 3.2294 (3.2300)  class_acc: 0.2891 (0.2893)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.7365 (30.9186)\n",
      "Epoch: [41]  [ 0/18]  eta: 0:00:44  lr: 0.000070  min_lr: 0.000070  loss: 3.1644 (3.1644)  class_acc: 0.3047 (0.3047)  weight_decay: 0.0500 (0.0500)  time: 2.4644  data: 0.9142\n",
      "Epoch: [41]  [17/18]  eta: 0:00:02  lr: 0.000069  min_lr: 0.000069  loss: 3.2184 (3.2288)  class_acc: 0.2852 (0.2858)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0863 (31.6306)  time: 2.2234  data: 0.9568\n",
      "Epoch: [41] Total time: 0:00:40 (2.2236 s / it)\n",
      "Averaged stats: lr: 0.000069  min_lr: 0.000069  loss: 3.2184 (3.2288)  class_acc: 0.2852 (0.2858)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.0863 (31.6306)\n",
      "Epoch: [42]  [ 0/18]  eta: 0:00:47  lr: 0.000069  min_lr: 0.000069  loss: 3.1881 (3.1881)  class_acc: 0.3516 (0.3516)  weight_decay: 0.0500 (0.0500)  time: 2.6436  data: 0.9382\n",
      "Epoch: [42]  [17/18]  eta: 0:00:02  lr: 0.000067  min_lr: 0.000067  loss: 3.2068 (3.2090)  class_acc: 0.2969 (0.3010)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2686 (31.4182)  time: 2.0012  data: 0.9429\n",
      "Epoch: [42] Total time: 0:00:36 (2.0013 s / it)\n",
      "Averaged stats: lr: 0.000067  min_lr: 0.000067  loss: 3.2068 (3.2090)  class_acc: 0.2969 (0.3010)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2686 (31.4182)\n",
      "Epoch: [43]  [ 0/18]  eta: 0:00:44  lr: 0.000067  min_lr: 0.000067  loss: 3.1061 (3.1061)  class_acc: 0.3242 (0.3242)  weight_decay: 0.0500 (0.0500)  time: 2.4782  data: 0.9454\n",
      "Epoch: [43]  [17/18]  eta: 0:00:02  lr: 0.000066  min_lr: 0.000066  loss: 3.1962 (3.1988)  class_acc: 0.2930 (0.2949)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.7064 (30.7100)  time: 2.0618  data: 0.9597\n",
      "Epoch: [43] Total time: 0:00:37 (2.0620 s / it)\n",
      "Averaged stats: lr: 0.000066  min_lr: 0.000066  loss: 3.1962 (3.1988)  class_acc: 0.2930 (0.2949)  weight_decay: 0.0500 (0.0500)  grad_norm: 29.7064 (30.7100)\n",
      "Epoch: [44]  [ 0/18]  eta: 0:00:38  lr: 0.000066  min_lr: 0.000066  loss: 3.0846 (3.0846)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.1602  data: 0.9205\n",
      "Epoch: [44]  [17/18]  eta: 0:00:02  lr: 0.000065  min_lr: 0.000065  loss: 3.1713 (3.1863)  class_acc: 0.3047 (0.3030)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.5387 (32.6894)  time: 2.0710  data: 0.9512\n",
      "Epoch: [44] Total time: 0:00:37 (2.0712 s / it)\n",
      "Averaged stats: lr: 0.000065  min_lr: 0.000065  loss: 3.1713 (3.1863)  class_acc: 0.3047 (0.3030)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.5387 (32.6894)\n",
      "Epoch: [45]  [ 0/18]  eta: 0:00:42  lr: 0.000064  min_lr: 0.000064  loss: 3.1330 (3.1330)  class_acc: 0.3281 (0.3281)  weight_decay: 0.0500 (0.0500)  time: 2.3453  data: 1.0095\n",
      "Epoch: [45]  [17/18]  eta: 0:00:02  lr: 0.000063  min_lr: 0.000063  loss: 3.1754 (3.1741)  class_acc: 0.3086 (0.3071)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.9002 (30.8614)  time: 2.1260  data: 0.9532\n",
      "Epoch: [45] Total time: 0:00:38 (2.1262 s / it)\n",
      "Averaged stats: lr: 0.000063  min_lr: 0.000063  loss: 3.1754 (3.1741)  class_acc: 0.3086 (0.3071)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.9002 (30.8614)\n",
      "Epoch: [46]  [ 0/18]  eta: 0:00:39  lr: 0.000063  min_lr: 0.000063  loss: 3.1226 (3.1226)  class_acc: 0.3438 (0.3438)  weight_decay: 0.0500 (0.0500)  time: 2.1851  data: 0.9425\n",
      "Epoch: [46]  [17/18]  eta: 0:00:02  lr: 0.000062  min_lr: 0.000062  loss: 3.1835 (3.1826)  class_acc: 0.3086 (0.3090)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8469 (30.8892)  time: 2.0068  data: 0.9412\n",
      "Epoch: [46] Total time: 0:00:36 (2.0069 s / it)\n",
      "Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 3.1835 (3.1826)  class_acc: 0.3086 (0.3090)  weight_decay: 0.0500 (0.0500)  grad_norm: 30.8469 (30.8892)\n",
      "Epoch: [47]  [ 0/18]  eta: 0:00:42  lr: 0.000062  min_lr: 0.000062  loss: 3.1436 (3.1436)  class_acc: 0.3008 (0.3008)  weight_decay: 0.0500 (0.0500)  time: 2.3723  data: 0.9140\n",
      "Epoch: [47]  [17/18]  eta: 0:00:02  lr: 0.000060  min_lr: 0.000060  loss: 3.1436 (3.1685)  class_acc: 0.2891 (0.3049)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.1209 (32.5679)  time: 2.2579  data: 0.9567\n",
      "Epoch: [47] Total time: 0:00:40 (2.2580 s / it)\n",
      "Averaged stats: lr: 0.000060  min_lr: 0.000060  loss: 3.1436 (3.1685)  class_acc: 0.2891 (0.3049)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.1209 (32.5679)\n",
      "Epoch: [48]  [ 0/18]  eta: 0:00:39  lr: 0.000060  min_lr: 0.000060  loss: 3.1240 (3.1240)  class_acc: 0.2656 (0.2656)  weight_decay: 0.0500 (0.0500)  time: 2.1961  data: 0.9126\n",
      "Epoch: [48]  [17/18]  eta: 0:00:02  lr: 0.000059  min_lr: 0.000059  loss: 3.1579 (3.1744)  class_acc: 0.2852 (0.2932)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.6226 (31.7599)  time: 2.3506  data: 0.9826\n",
      "Epoch: [48] Total time: 0:00:42 (2.3511 s / it)\n",
      "Averaged stats: lr: 0.000059  min_lr: 0.000059  loss: 3.1579 (3.1744)  class_acc: 0.2852 (0.2932)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.6226 (31.7599)\n",
      "Epoch: [49]  [ 0/18]  eta: 0:00:49  lr: 0.000059  min_lr: 0.000059  loss: 3.1674 (3.1674)  class_acc: 0.3359 (0.3359)  weight_decay: 0.0500 (0.0500)  time: 2.7376  data: 0.9187\n",
      "Epoch: [49]  [17/18]  eta: 0:00:02  lr: 0.000057  min_lr: 0.000057  loss: 3.1404 (3.1432)  class_acc: 0.3164 (0.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.1190 (32.0898)  time: 2.2929  data: 0.9776\n",
      "Epoch: [49] Total time: 0:00:41 (2.2931 s / it)\n",
      "Averaged stats: lr: 0.000057  min_lr: 0.000057  loss: 3.1404 (3.1432)  class_acc: 0.3164 (0.3121)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.1190 (32.0898)\n",
      "Epoch: [50]  [ 0/18]  eta: 0:00:37  lr: 0.000057  min_lr: 0.000057  loss: 3.1366 (3.1366)  class_acc: 0.3047 (0.3047)  weight_decay: 0.0500 (0.0500)  time: 2.0840  data: 0.8956\n",
      "Epoch: [50]  [17/18]  eta: 0:00:02  lr: 0.000056  min_lr: 0.000056  loss: 3.1331 (3.1316)  class_acc: 0.3125 (0.3188)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.7664 (32.0398)  time: 2.1873  data: 0.9712\n",
      "Epoch: [50] Total time: 0:00:39 (2.1874 s / it)\n",
      "Averaged stats: lr: 0.000056  min_lr: 0.000056  loss: 3.1331 (3.1316)  class_acc: 0.3125 (0.3188)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.7664 (32.0398)\n",
      "Epoch: [51]  [ 0/18]  eta: 0:00:41  lr: 0.000056  min_lr: 0.000056  loss: 3.0953 (3.0953)  class_acc: 0.3438 (0.3438)  weight_decay: 0.0500 (0.0500)  time: 2.2869  data: 0.9740\n",
      "Epoch: [51]  [17/18]  eta: 0:00:02  lr: 0.000054  min_lr: 0.000054  loss: 3.1430 (3.1390)  class_acc: 0.3164 (0.3151)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.6680 (31.8248)  time: 2.3101  data: 1.0162\n",
      "Epoch: [51] Total time: 0:00:41 (2.3102 s / it)\n",
      "Averaged stats: lr: 0.000054  min_lr: 0.000054  loss: 3.1430 (3.1390)  class_acc: 0.3164 (0.3151)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.6680 (31.8248)\n",
      "Epoch: [52]  [ 0/18]  eta: 0:00:40  lr: 0.000054  min_lr: 0.000054  loss: 3.0368 (3.0368)  class_acc: 0.3516 (0.3516)  weight_decay: 0.0500 (0.0500)  time: 2.2659  data: 0.9134\n",
      "Epoch: [52]  [17/18]  eta: 0:00:02  lr: 0.000053  min_lr: 0.000053  loss: 3.1272 (3.1276)  class_acc: 0.3164 (0.3149)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0202 (32.3670)  time: 2.0542  data: 0.9547\n",
      "Epoch: [52] Total time: 0:00:36 (2.0543 s / it)\n",
      "Averaged stats: lr: 0.000053  min_lr: 0.000053  loss: 3.1272 (3.1276)  class_acc: 0.3164 (0.3149)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0202 (32.3670)\n",
      "Epoch: [53]  [ 0/18]  eta: 0:00:41  lr: 0.000053  min_lr: 0.000053  loss: 3.0492 (3.0492)  class_acc: 0.3477 (0.3477)  weight_decay: 0.0500 (0.0500)  time: 2.2922  data: 0.9983\n",
      "Epoch: [53]  [17/18]  eta: 0:00:02  lr: 0.000052  min_lr: 0.000052  loss: 3.1066 (3.1191)  class_acc: 0.3203 (0.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.9368 (31.7552)  time: 2.0738  data: 0.9634\n",
      "Epoch: [53] Total time: 0:00:37 (2.0739 s / it)\n",
      "Averaged stats: lr: 0.000052  min_lr: 0.000052  loss: 3.1066 (3.1191)  class_acc: 0.3203 (0.3223)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.9368 (31.7552)\n",
      "Epoch: [54]  [ 0/18]  eta: 0:00:38  lr: 0.000051  min_lr: 0.000051  loss: 3.0469 (3.0469)  class_acc: 0.3281 (0.3281)  weight_decay: 0.0500 (0.0500)  time: 2.1269  data: 0.9389\n",
      "Epoch: [54]  [17/18]  eta: 0:00:02  lr: 0.000050  min_lr: 0.000050  loss: 3.1043 (3.1103)  class_acc: 0.3242 (0.3266)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.7929 (32.0202)  time: 2.0619  data: 0.9580\n",
      "Epoch: [54] Total time: 0:00:37 (2.0620 s / it)\n",
      "Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 3.1043 (3.1103)  class_acc: 0.3242 (0.3266)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.7929 (32.0202)\n",
      "Epoch: [55]  [ 0/18]  eta: 0:00:39  lr: 0.000050  min_lr: 0.000050  loss: 3.0306 (3.0306)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.1797  data: 1.0144\n",
      "Epoch: [55]  [17/18]  eta: 0:00:02  lr: 0.000049  min_lr: 0.000049  loss: 3.1055 (3.1120)  class_acc: 0.3242 (0.3207)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2944 (31.7029)  time: 2.0999  data: 0.9657\n",
      "Epoch: [55] Total time: 0:00:37 (2.1001 s / it)\n",
      "Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 3.1055 (3.1120)  class_acc: 0.3242 (0.3207)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.2944 (31.7029)\n",
      "Epoch: [56]  [ 0/18]  eta: 0:00:42  lr: 0.000048  min_lr: 0.000048  loss: 3.1175 (3.1175)  class_acc: 0.3320 (0.3320)  weight_decay: 0.0500 (0.0500)  time: 2.3418  data: 0.9257\n",
      "Epoch: [56]  [17/18]  eta: 0:00:02  lr: 0.000047  min_lr: 0.000047  loss: 3.1175 (3.1098)  class_acc: 0.3203 (0.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2298 (32.9121)  time: 2.0805  data: 0.9516\n",
      "Epoch: [56] Total time: 0:00:37 (2.0806 s / it)\n",
      "Averaged stats: lr: 0.000047  min_lr: 0.000047  loss: 3.1175 (3.1098)  class_acc: 0.3203 (0.3262)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2298 (32.9121)\n",
      "Epoch: [57]  [ 0/18]  eta: 0:00:43  lr: 0.000047  min_lr: 0.000047  loss: 3.0822 (3.0822)  class_acc: 0.3438 (0.3438)  weight_decay: 0.0500 (0.0500)  time: 2.4238  data: 0.9264\n",
      "Epoch: [57]  [17/18]  eta: 0:00:02  lr: 0.000046  min_lr: 0.000046  loss: 3.0822 (3.0953)  class_acc: 0.3320 (0.3340)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.2698 (32.2518)  time: 2.3849  data: 0.9552\n",
      "Epoch: [57] Total time: 0:00:42 (2.3852 s / it)\n",
      "Averaged stats: lr: 0.000046  min_lr: 0.000046  loss: 3.0822 (3.0953)  class_acc: 0.3320 (0.3340)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.2698 (32.2518)\n",
      "Epoch: [58]  [ 0/18]  eta: 0:00:45  lr: 0.000046  min_lr: 0.000046  loss: 3.0195 (3.0195)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.5016  data: 0.9563\n",
      "Epoch: [58]  [17/18]  eta: 0:00:02  lr: 0.000044  min_lr: 0.000044  loss: 3.0874 (3.0845)  class_acc: 0.3203 (0.3270)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.9946 (33.1129)  time: 2.1738  data: 0.9628\n",
      "Epoch: [58] Total time: 0:00:39 (2.1739 s / it)\n",
      "Averaged stats: lr: 0.000044  min_lr: 0.000044  loss: 3.0874 (3.0845)  class_acc: 0.3203 (0.3270)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.9946 (33.1129)\n",
      "Epoch: [59]  [ 0/18]  eta: 0:00:39  lr: 0.000044  min_lr: 0.000044  loss: 3.1044 (3.1044)  class_acc: 0.3008 (0.3008)  weight_decay: 0.0500 (0.0500)  time: 2.1988  data: 0.9318\n",
      "Epoch: [59]  [17/18]  eta: 0:00:02  lr: 0.000043  min_lr: 0.000043  loss: 3.0755 (3.0770)  class_acc: 0.3281 (0.3336)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.6365 (32.4959)  time: 2.1169  data: 0.9516\n",
      "Epoch: [59] Total time: 0:00:38 (2.1170 s / it)\n",
      "Averaged stats: lr: 0.000043  min_lr: 0.000043  loss: 3.0755 (3.0770)  class_acc: 0.3281 (0.3336)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.6365 (32.4959)\n",
      "Epoch: [60]  [ 0/18]  eta: 0:00:40  lr: 0.000043  min_lr: 0.000043  loss: 3.0210 (3.0210)  class_acc: 0.3555 (0.3555)  weight_decay: 0.0500 (0.0500)  time: 2.2694  data: 0.9211\n",
      "Epoch: [60]  [17/18]  eta: 0:00:02  lr: 0.000042  min_lr: 0.000042  loss: 3.0965 (3.0883)  class_acc: 0.3320 (0.3381)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.8007 (31.8715)  time: 2.0664  data: 0.9619\n",
      "Epoch: [60] Total time: 0:00:37 (2.0665 s / it)\n",
      "Averaged stats: lr: 0.000042  min_lr: 0.000042  loss: 3.0965 (3.0883)  class_acc: 0.3320 (0.3381)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.8007 (31.8715)\n",
      "Epoch: [61]  [ 0/18]  eta: 0:00:43  lr: 0.000041  min_lr: 0.000041  loss: 3.0277 (3.0277)  class_acc: 0.3125 (0.3125)  weight_decay: 0.0500 (0.0500)  time: 2.3979  data: 0.9989\n",
      "Epoch: [61]  [17/18]  eta: 0:00:02  lr: 0.000040  min_lr: 0.000040  loss: 3.0434 (3.0429)  class_acc: 0.3438 (0.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.3462 (31.2422)  time: 2.3104  data: 0.9544\n",
      "Epoch: [61] Total time: 0:00:41 (2.3109 s / it)\n",
      "Averaged stats: lr: 0.000040  min_lr: 0.000040  loss: 3.0434 (3.0429)  class_acc: 0.3438 (0.3457)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.3462 (31.2422)\n",
      "Epoch: [62]  [ 0/18]  eta: 0:00:44  lr: 0.000040  min_lr: 0.000040  loss: 2.9596 (2.9596)  class_acc: 0.3750 (0.3750)  weight_decay: 0.0500 (0.0500)  time: 2.4755  data: 0.9128\n",
      "Epoch: [62]  [17/18]  eta: 0:00:02  lr: 0.000039  min_lr: 0.000039  loss: 3.0220 (3.0559)  class_acc: 0.3242 (0.3331)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.1759 (33.1850)  time: 2.4385  data: 0.9691\n",
      "Epoch: [62] Total time: 0:00:43 (2.4389 s / it)\n",
      "Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 3.0220 (3.0559)  class_acc: 0.3242 (0.3331)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.1759 (33.1850)\n",
      "Epoch: [63]  [ 0/18]  eta: 0:00:43  lr: 0.000039  min_lr: 0.000039  loss: 3.0063 (3.0063)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.4075  data: 0.9239\n",
      "Epoch: [63]  [17/18]  eta: 0:00:02  lr: 0.000037  min_lr: 0.000037  loss: 3.0392 (3.0519)  class_acc: 0.3438 (0.3405)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7126 (33.6294)  time: 2.1666  data: 0.9599\n",
      "Epoch: [63] Total time: 0:00:38 (2.1666 s / it)\n",
      "Averaged stats: lr: 0.000037  min_lr: 0.000037  loss: 3.0392 (3.0519)  class_acc: 0.3438 (0.3405)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7126 (33.6294)\n",
      "Epoch: [64]  [ 0/18]  eta: 0:00:42  lr: 0.000037  min_lr: 0.000037  loss: 3.0205 (3.0205)  class_acc: 0.3633 (0.3633)  weight_decay: 0.0500 (0.0500)  time: 2.3763  data: 0.9587\n",
      "Epoch: [64]  [17/18]  eta: 0:00:02  lr: 0.000036  min_lr: 0.000036  loss: 3.0349 (3.0405)  class_acc: 0.3438 (0.3424)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.4194 (32.6991)  time: 2.0620  data: 0.9479\n",
      "Epoch: [64] Total time: 0:00:37 (2.0622 s / it)\n",
      "Averaged stats: lr: 0.000036  min_lr: 0.000036  loss: 3.0349 (3.0405)  class_acc: 0.3438 (0.3424)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.4194 (32.6991)\n",
      "Epoch: [65]  [ 0/18]  eta: 0:00:40  lr: 0.000036  min_lr: 0.000036  loss: 2.9567 (2.9567)  class_acc: 0.3359 (0.3359)  weight_decay: 0.0500 (0.0500)  time: 2.2623  data: 0.9001\n",
      "Epoch: [65]  [17/18]  eta: 0:00:02  lr: 0.000035  min_lr: 0.000035  loss: 3.0290 (3.0388)  class_acc: 0.3359 (0.3407)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0221 (32.7281)  time: 2.0953  data: 0.9633\n",
      "Epoch: [65] Total time: 0:00:37 (2.0954 s / it)\n",
      "Averaged stats: lr: 0.000035  min_lr: 0.000035  loss: 3.0290 (3.0388)  class_acc: 0.3359 (0.3407)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0221 (32.7281)\n",
      "Epoch: [66]  [ 0/18]  eta: 0:00:35  lr: 0.000035  min_lr: 0.000035  loss: 2.9025 (2.9025)  class_acc: 0.3828 (0.3828)  weight_decay: 0.0500 (0.0500)  time: 1.9774  data: 0.8798\n",
      "Epoch: [66]  [17/18]  eta: 0:00:02  lr: 0.000033  min_lr: 0.000033  loss: 3.0265 (3.0359)  class_acc: 0.3242 (0.3377)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8857 (32.8221)  time: 2.1332  data: 0.9537\n",
      "Epoch: [66] Total time: 0:00:38 (2.1333 s / it)\n",
      "Averaged stats: lr: 0.000033  min_lr: 0.000033  loss: 3.0265 (3.0359)  class_acc: 0.3242 (0.3377)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8857 (32.8221)\n",
      "Epoch: [67]  [ 0/18]  eta: 0:00:44  lr: 0.000033  min_lr: 0.000033  loss: 2.9498 (2.9498)  class_acc: 0.4062 (0.4062)  weight_decay: 0.0500 (0.0500)  time: 2.4724  data: 1.0086\n",
      "Epoch: [67]  [17/18]  eta: 0:00:02  lr: 0.000032  min_lr: 0.000032  loss: 3.0394 (3.0473)  class_acc: 0.3359 (0.3377)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2042 (32.6766)  time: 2.0717  data: 0.9483\n",
      "Epoch: [67] Total time: 0:00:37 (2.0719 s / it)\n",
      "Averaged stats: lr: 0.000032  min_lr: 0.000032  loss: 3.0394 (3.0473)  class_acc: 0.3359 (0.3377)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2042 (32.6766)\n",
      "Epoch: [68]  [ 0/18]  eta: 0:00:39  lr: 0.000032  min_lr: 0.000032  loss: 2.9600 (2.9600)  class_acc: 0.3672 (0.3672)  weight_decay: 0.0500 (0.0500)  time: 2.1731  data: 0.9177\n",
      "Epoch: [68]  [17/18]  eta: 0:00:02  lr: 0.000031  min_lr: 0.000031  loss: 3.0410 (3.0256)  class_acc: 0.3477 (0.3550)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.4253 (33.0062)  time: 2.1065  data: 0.9437\n",
      "Epoch: [68] Total time: 0:00:37 (2.1066 s / it)\n",
      "Averaged stats: lr: 0.000031  min_lr: 0.000031  loss: 3.0410 (3.0256)  class_acc: 0.3477 (0.3550)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.4253 (33.0062)\n",
      "Epoch: [69]  [ 0/18]  eta: 0:01:01  lr: 0.000031  min_lr: 0.000031  loss: 2.9664 (2.9664)  class_acc: 0.3438 (0.3438)  weight_decay: 0.0500 (0.0500)  time: 3.4401  data: 0.9467\n",
      "Epoch: [69]  [17/18]  eta: 0:00:02  lr: 0.000030  min_lr: 0.000030  loss: 3.0321 (3.0215)  class_acc: 0.3398 (0.3431)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.5478 (33.7830)  time: 2.2389  data: 0.9536\n",
      "Epoch: [69] Total time: 0:00:40 (2.2395 s / it)\n",
      "Averaged stats: lr: 0.000030  min_lr: 0.000030  loss: 3.0321 (3.0215)  class_acc: 0.3398 (0.3431)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.5478 (33.7830)\n",
      "Epoch: [70]  [ 0/18]  eta: 0:00:49  lr: 0.000030  min_lr: 0.000030  loss: 3.0039 (3.0039)  class_acc: 0.3906 (0.3906)  weight_decay: 0.0500 (0.0500)  time: 2.7466  data: 0.9852\n",
      "Epoch: [70]  [17/18]  eta: 0:00:02  lr: 0.000029  min_lr: 0.000029  loss: 3.0126 (3.0133)  class_acc: 0.3438 (0.3516)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7726 (32.9031)  time: 2.1557  data: 0.9726\n",
      "Epoch: [70] Total time: 0:00:38 (2.1559 s / it)\n",
      "Averaged stats: lr: 0.000029  min_lr: 0.000029  loss: 3.0126 (3.0133)  class_acc: 0.3438 (0.3516)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7726 (32.9031)\n",
      "Epoch: [71]  [ 0/18]  eta: 0:00:44  lr: 0.000028  min_lr: 0.000028  loss: 2.9873 (2.9873)  class_acc: 0.3633 (0.3633)  weight_decay: 0.0500 (0.0500)  time: 2.4736  data: 0.9536\n",
      "Epoch: [71]  [17/18]  eta: 0:00:02  lr: 0.000027  min_lr: 0.000027  loss: 3.0032 (3.0118)  class_acc: 0.3477 (0.3487)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.8389 (32.4216)  time: 2.0900  data: 0.9572\n",
      "Epoch: [71] Total time: 0:00:37 (2.0902 s / it)\n",
      "Averaged stats: lr: 0.000027  min_lr: 0.000027  loss: 3.0032 (3.0118)  class_acc: 0.3477 (0.3487)  weight_decay: 0.0500 (0.0500)  grad_norm: 31.8389 (32.4216)\n",
      "Epoch: [72]  [ 0/18]  eta: 0:00:41  lr: 0.000027  min_lr: 0.000027  loss: 2.9486 (2.9486)  class_acc: 0.3672 (0.3672)  weight_decay: 0.0500 (0.0500)  time: 2.2811  data: 0.9901\n",
      "Epoch: [72]  [17/18]  eta: 0:00:02  lr: 0.000026  min_lr: 0.000026  loss: 3.0327 (3.0352)  class_acc: 0.3359 (0.3401)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.1651 (32.9299)  time: 2.0592  data: 0.9550\n",
      "Epoch: [72] Total time: 0:00:37 (2.0593 s / it)\n",
      "Averaged stats: lr: 0.000026  min_lr: 0.000026  loss: 3.0327 (3.0352)  class_acc: 0.3359 (0.3401)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.1651 (32.9299)\n",
      "Epoch: [73]  [ 0/18]  eta: 0:00:46  lr: 0.000026  min_lr: 0.000026  loss: 2.9325 (2.9325)  class_acc: 0.3906 (0.3906)  weight_decay: 0.0500 (0.0500)  time: 2.5562  data: 0.9087\n",
      "Epoch: [73]  [17/18]  eta: 0:00:02  lr: 0.000025  min_lr: 0.000025  loss: 3.0263 (3.0126)  class_acc: 0.3477 (0.3505)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7983 (33.0410)  time: 2.1546  data: 0.9588\n",
      "Epoch: [73] Total time: 0:00:38 (2.1549 s / it)\n",
      "Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 3.0263 (3.0126)  class_acc: 0.3477 (0.3505)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7983 (33.0410)\n",
      "Epoch: [74]  [ 0/18]  eta: 0:00:43  lr: 0.000025  min_lr: 0.000025  loss: 2.9880 (2.9880)  class_acc: 0.3516 (0.3516)  weight_decay: 0.0500 (0.0500)  time: 2.3892  data: 0.9371\n",
      "Epoch: [74]  [17/18]  eta: 0:00:02  lr: 0.000024  min_lr: 0.000024  loss: 3.0006 (3.0216)  class_acc: 0.3516 (0.3477)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.9995 (33.3100)  time: 2.2611  data: 0.9665\n",
      "Epoch: [74] Total time: 0:00:40 (2.2612 s / it)\n",
      "Averaged stats: lr: 0.000024  min_lr: 0.000024  loss: 3.0006 (3.0216)  class_acc: 0.3516 (0.3477)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.9995 (33.3100)\n",
      "Epoch: [75]  [ 0/18]  eta: 0:00:43  lr: 0.000024  min_lr: 0.000024  loss: 3.0570 (3.0570)  class_acc: 0.3008 (0.3008)  weight_decay: 0.0500 (0.0500)  time: 2.4283  data: 0.9223\n",
      "Epoch: [75]  [17/18]  eta: 0:00:02  lr: 0.000023  min_lr: 0.000023  loss: 3.0056 (3.0270)  class_acc: 0.3516 (0.3485)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2336 (33.2602)  time: 2.0788  data: 0.9512\n",
      "Epoch: [75] Total time: 0:00:37 (2.0789 s / it)\n",
      "Averaged stats: lr: 0.000023  min_lr: 0.000023  loss: 3.0056 (3.0270)  class_acc: 0.3516 (0.3485)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2336 (33.2602)\n",
      "Epoch: [76]  [ 0/18]  eta: 0:00:46  lr: 0.000023  min_lr: 0.000023  loss: 2.9781 (2.9781)  class_acc: 0.3672 (0.3672)  weight_decay: 0.0500 (0.0500)  time: 2.5903  data: 0.9874\n",
      "Epoch: [76]  [17/18]  eta: 0:00:02  lr: 0.000022  min_lr: 0.000022  loss: 3.0142 (3.0218)  class_acc: 0.3477 (0.3459)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.8735 (33.6679)  time: 2.1087  data: 0.9623\n",
      "Epoch: [76] Total time: 0:00:37 (2.1089 s / it)\n",
      "Averaged stats: lr: 0.000022  min_lr: 0.000022  loss: 3.0142 (3.0218)  class_acc: 0.3477 (0.3459)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.8735 (33.6679)\n",
      "Epoch: [77]  [ 0/18]  eta: 0:00:38  lr: 0.000022  min_lr: 0.000022  loss: 2.9444 (2.9444)  class_acc: 0.3750 (0.3750)  weight_decay: 0.0500 (0.0500)  time: 2.1193  data: 0.9336\n",
      "Epoch: [77]  [17/18]  eta: 0:00:02  lr: 0.000021  min_lr: 0.000021  loss: 2.9974 (2.9995)  class_acc: 0.3516 (0.3596)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8441 (32.7532)  time: 2.0404  data: 0.9502\n",
      "Epoch: [77] Total time: 0:00:36 (2.0405 s / it)\n",
      "Averaged stats: lr: 0.000021  min_lr: 0.000021  loss: 2.9974 (2.9995)  class_acc: 0.3516 (0.3596)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8441 (32.7532)\n",
      "Epoch: [78]  [ 0/18]  eta: 0:00:38  lr: 0.000021  min_lr: 0.000021  loss: 2.9803 (2.9803)  class_acc: 0.3555 (0.3555)  weight_decay: 0.0500 (0.0500)  time: 2.1322  data: 0.9466\n",
      "Epoch: [78]  [17/18]  eta: 0:00:02  lr: 0.000020  min_lr: 0.000020  loss: 2.9807 (3.0033)  class_acc: 0.3555 (0.3542)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9767 (33.9414)  time: 2.5195  data: 1.0289\n",
      "Epoch: [78] Total time: 0:00:45 (2.5199 s / it)\n",
      "Averaged stats: lr: 0.000020  min_lr: 0.000020  loss: 2.9807 (3.0033)  class_acc: 0.3555 (0.3542)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9767 (33.9414)\n",
      "Epoch: [79]  [ 0/18]  eta: 0:00:44  lr: 0.000020  min_lr: 0.000020  loss: 2.9675 (2.9675)  class_acc: 0.3633 (0.3633)  weight_decay: 0.0500 (0.0500)  time: 2.4930  data: 0.9377\n",
      "Epoch: [79]  [17/18]  eta: 0:00:02  lr: 0.000019  min_lr: 0.000019  loss: 2.9897 (2.9914)  class_acc: 0.3438 (0.3505)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6841 (33.4707)  time: 2.2056  data: 0.9653\n",
      "Epoch: [79] Total time: 0:00:39 (2.2057 s / it)\n",
      "Averaged stats: lr: 0.000019  min_lr: 0.000019  loss: 2.9897 (2.9914)  class_acc: 0.3438 (0.3505)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.6841 (33.4707)\n",
      "Epoch: [80]  [ 0/18]  eta: 0:00:43  lr: 0.000019  min_lr: 0.000019  loss: 2.9860 (2.9860)  class_acc: 0.3672 (0.3672)  weight_decay: 0.0500 (0.0500)  time: 2.4140  data: 0.9644\n",
      "Epoch: [80]  [17/18]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000018  loss: 3.0133 (2.9992)  class_acc: 0.3516 (0.3544)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.5147 (33.3637)  time: 2.1565  data: 0.9536\n",
      "Epoch: [80] Total time: 0:00:38 (2.1566 s / it)\n",
      "Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 3.0133 (2.9992)  class_acc: 0.3516 (0.3544)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.5147 (33.3637)\n",
      "Epoch: [81]  [ 0/18]  eta: 0:00:48  lr: 0.000018  min_lr: 0.000018  loss: 2.9846 (2.9846)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.6801  data: 0.9336\n",
      "Epoch: [81]  [17/18]  eta: 0:00:02  lr: 0.000018  min_lr: 0.000018  loss: 2.9846 (2.9856)  class_acc: 0.3594 (0.3555)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2383 (32.9290)  time: 2.2281  data: 0.9692\n",
      "Epoch: [81] Total time: 0:00:40 (2.2282 s / it)\n",
      "Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 2.9846 (2.9856)  class_acc: 0.3594 (0.3555)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2383 (32.9290)\n",
      "Epoch: [82]  [ 0/18]  eta: 0:00:52  lr: 0.000017  min_lr: 0.000017  loss: 2.9723 (2.9723)  class_acc: 0.3750 (0.3750)  weight_decay: 0.0500 (0.0500)  time: 2.9196  data: 0.9622\n",
      "Epoch: [82]  [17/18]  eta: 0:00:02  lr: 0.000017  min_lr: 0.000017  loss: 3.0025 (3.0184)  class_acc: 0.3594 (0.3518)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7294 (33.3149)  time: 2.1495  data: 0.9699\n",
      "Epoch: [82] Total time: 0:00:38 (2.1497 s / it)\n",
      "Averaged stats: lr: 0.000017  min_lr: 0.000017  loss: 3.0025 (3.0184)  class_acc: 0.3594 (0.3518)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7294 (33.3149)\n",
      "Epoch: [83]  [ 0/18]  eta: 0:00:43  lr: 0.000017  min_lr: 0.000017  loss: 3.0195 (3.0195)  class_acc: 0.3477 (0.3477)  weight_decay: 0.0500 (0.0500)  time: 2.4123  data: 0.9249\n",
      "Epoch: [83]  [17/18]  eta: 0:00:02  lr: 0.000016  min_lr: 0.000016  loss: 3.0138 (2.9882)  class_acc: 0.3477 (0.3624)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3378 (32.9139)  time: 2.1165  data: 0.9581\n",
      "Epoch: [83] Total time: 0:00:38 (2.1168 s / it)\n",
      "Averaged stats: lr: 0.000016  min_lr: 0.000016  loss: 3.0138 (2.9882)  class_acc: 0.3477 (0.3624)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3378 (32.9139)\n",
      "Epoch: [84]  [ 0/18]  eta: 0:00:38  lr: 0.000016  min_lr: 0.000016  loss: 2.9163 (2.9163)  class_acc: 0.3906 (0.3906)  weight_decay: 0.0500 (0.0500)  time: 2.1290  data: 0.9957\n",
      "Epoch: [84]  [17/18]  eta: 0:00:02  lr: 0.000015  min_lr: 0.000015  loss: 3.0042 (2.9997)  class_acc: 0.3438 (0.3490)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3556 (33.2914)  time: 2.0907  data: 0.9594\n",
      "Epoch: [84] Total time: 0:00:37 (2.0908 s / it)\n",
      "Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 3.0042 (2.9997)  class_acc: 0.3438 (0.3490)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3556 (33.2914)\n",
      "Epoch: [85]  [ 0/18]  eta: 0:00:55  lr: 0.000015  min_lr: 0.000015  loss: 2.9609 (2.9609)  class_acc: 0.3906 (0.3906)  weight_decay: 0.0500 (0.0500)  time: 3.1096  data: 0.9523\n",
      "Epoch: [85]  [17/18]  eta: 0:00:02  lr: 0.000015  min_lr: 0.000015  loss: 2.9653 (2.9765)  class_acc: 0.3633 (0.3657)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8568 (33.4217)  time: 2.1999  data: 0.9697\n",
      "Epoch: [85] Total time: 0:00:39 (2.2001 s / it)\n",
      "Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 2.9653 (2.9765)  class_acc: 0.3633 (0.3657)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.8568 (33.4217)\n",
      "Epoch: [86]  [ 0/18]  eta: 0:00:40  lr: 0.000015  min_lr: 0.000015  loss: 2.9555 (2.9555)  class_acc: 0.4023 (0.4023)  weight_decay: 0.0500 (0.0500)  time: 2.2275  data: 0.9543\n",
      "Epoch: [86]  [17/18]  eta: 0:00:02  lr: 0.000014  min_lr: 0.000014  loss: 2.9555 (2.9759)  class_acc: 0.3555 (0.3589)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9631 (33.4946)  time: 2.1666  data: 0.9998\n",
      "Epoch: [86] Total time: 0:00:39 (2.1667 s / it)\n",
      "Averaged stats: lr: 0.000014  min_lr: 0.000014  loss: 2.9555 (2.9759)  class_acc: 0.3555 (0.3589)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9631 (33.4946)\n",
      "Epoch: [87]  [ 0/18]  eta: 0:00:40  lr: 0.000014  min_lr: 0.000014  loss: 2.8881 (2.8881)  class_acc: 0.3945 (0.3945)  weight_decay: 0.0500 (0.0500)  time: 2.2762  data: 0.9347\n",
      "Epoch: [87]  [17/18]  eta: 0:00:02  lr: 0.000013  min_lr: 0.000013  loss: 2.9644 (2.9748)  class_acc: 0.3672 (0.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.4558 (32.9770)  time: 2.1856  data: 0.9641\n",
      "Epoch: [87] Total time: 0:00:39 (2.1857 s / it)\n",
      "Averaged stats: lr: 0.000013  min_lr: 0.000013  loss: 2.9644 (2.9748)  class_acc: 0.3672 (0.3659)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.4558 (32.9770)\n",
      "Epoch: [88]  [ 0/18]  eta: 0:00:42  lr: 0.000013  min_lr: 0.000013  loss: 2.9815 (2.9815)  class_acc: 0.3555 (0.3555)  weight_decay: 0.0500 (0.0500)  time: 2.3580  data: 0.9161\n",
      "Epoch: [88]  [17/18]  eta: 0:00:02  lr: 0.000013  min_lr: 0.000013  loss: 2.9771 (2.9806)  class_acc: 0.3555 (0.3579)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3112 (33.5925)  time: 2.0907  data: 0.9560\n",
      "Epoch: [88] Total time: 0:00:37 (2.0909 s / it)\n",
      "Averaged stats: lr: 0.000013  min_lr: 0.000013  loss: 2.9771 (2.9806)  class_acc: 0.3555 (0.3579)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.3112 (33.5925)\n",
      "Epoch: [89]  [ 0/18]  eta: 0:00:42  lr: 0.000013  min_lr: 0.000013  loss: 2.9517 (2.9517)  class_acc: 0.3906 (0.3906)  weight_decay: 0.0500 (0.0500)  time: 2.3608  data: 0.9403\n",
      "Epoch: [89]  [17/18]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000012  loss: 2.9639 (2.9677)  class_acc: 0.3633 (0.3639)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2048 (33.0882)  time: 2.1387  data: 0.9540\n",
      "Epoch: [89] Total time: 0:00:38 (2.1388 s / it)\n",
      "Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 2.9639 (2.9677)  class_acc: 0.3633 (0.3639)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.2048 (33.0882)\n",
      "Epoch: [90]  [ 0/18]  eta: 0:00:36  lr: 0.000012  min_lr: 0.000012  loss: 2.8350 (2.8350)  class_acc: 0.3945 (0.3945)  weight_decay: 0.0500 (0.0500)  time: 2.0201  data: 0.9102\n",
      "Epoch: [90]  [17/18]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000012  loss: 2.9656 (2.9712)  class_acc: 0.3555 (0.3570)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7169 (33.6078)  time: 2.1327  data: 0.9652\n",
      "Epoch: [90] Total time: 0:00:38 (2.1331 s / it)\n",
      "Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 2.9656 (2.9712)  class_acc: 0.3555 (0.3570)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7169 (33.6078)\n",
      "Epoch: [91]  [ 0/18]  eta: 0:00:39  lr: 0.000012  min_lr: 0.000012  loss: 2.8621 (2.8621)  class_acc: 0.4180 (0.4180)  weight_decay: 0.0500 (0.0500)  time: 2.1916  data: 0.9239\n",
      "Epoch: [91]  [17/18]  eta: 0:00:02  lr: 0.000012  min_lr: 0.000012  loss: 2.9672 (2.9579)  class_acc: 0.3672 (0.3811)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9447 (33.7990)  time: 2.2229  data: 0.9744\n",
      "Epoch: [91] Total time: 0:00:40 (2.2231 s / it)\n",
      "Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 2.9672 (2.9579)  class_acc: 0.3672 (0.3811)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9447 (33.7990)\n",
      "Epoch: [92]  [ 0/18]  eta: 0:00:44  lr: 0.000012  min_lr: 0.000012  loss: 2.8708 (2.8708)  class_acc: 0.3984 (0.3984)  weight_decay: 0.0500 (0.0500)  time: 2.4601  data: 0.9127\n",
      "Epoch: [92]  [17/18]  eta: 0:00:02  lr: 0.000011  min_lr: 0.000011  loss: 2.9390 (2.9591)  class_acc: 0.3789 (0.3728)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0675 (33.1054)  time: 2.0603  data: 0.9413\n",
      "Epoch: [92] Total time: 0:00:37 (2.0604 s / it)\n",
      "Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 2.9390 (2.9591)  class_acc: 0.3789 (0.3728)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0675 (33.1054)\n",
      "Epoch: [93]  [ 0/18]  eta: 0:00:47  lr: 0.000011  min_lr: 0.000011  loss: 2.9113 (2.9113)  class_acc: 0.3867 (0.3867)  weight_decay: 0.0500 (0.0500)  time: 2.6294  data: 0.9226\n",
      "Epoch: [93]  [17/18]  eta: 0:00:02  lr: 0.000011  min_lr: 0.000011  loss: 2.9629 (2.9650)  class_acc: 0.3633 (0.3672)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7803 (33.1654)  time: 2.1195  data: 0.9577\n",
      "Epoch: [93] Total time: 0:00:38 (2.1196 s / it)\n",
      "Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 2.9629 (2.9650)  class_acc: 0.3633 (0.3672)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.7803 (33.1654)\n",
      "Epoch: [94]  [ 0/18]  eta: 0:00:45  lr: 0.000011  min_lr: 0.000011  loss: 2.9842 (2.9842)  class_acc: 0.3789 (0.3789)  weight_decay: 0.0500 (0.0500)  time: 2.5440  data: 0.9583\n",
      "Epoch: [94]  [17/18]  eta: 0:00:02  lr: 0.000011  min_lr: 0.000011  loss: 2.9711 (2.9803)  class_acc: 0.3477 (0.3557)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.3823 (33.9638)  time: 2.0698  data: 0.9424\n",
      "Epoch: [94] Total time: 0:00:37 (2.0699 s / it)\n",
      "Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 2.9711 (2.9803)  class_acc: 0.3477 (0.3557)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.3823 (33.9638)\n",
      "Epoch: [95]  [ 0/18]  eta: 0:00:46  lr: 0.000011  min_lr: 0.000011  loss: 2.9097 (2.9097)  class_acc: 0.3828 (0.3828)  weight_decay: 0.0500 (0.0500)  time: 2.5746  data: 0.8966\n",
      "Epoch: [95]  [17/18]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.9476 (2.9533)  class_acc: 0.3633 (0.3657)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.0755 (33.8347)  time: 2.2377  data: 0.9746\n",
      "Epoch: [95] Total time: 0:00:40 (2.2381 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 2.9476 (2.9533)  class_acc: 0.3633 (0.3657)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.0755 (33.8347)\n",
      "Epoch: [96]  [ 0/18]  eta: 0:01:00  lr: 0.000010  min_lr: 0.000010  loss: 2.9281 (2.9281)  class_acc: 0.3945 (0.3945)  weight_decay: 0.0500 (0.0500)  time: 3.3587  data: 0.9250\n",
      "Epoch: [96]  [17/18]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.9811 (2.9705)  class_acc: 0.3633 (0.3683)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7239 (33.2576)  time: 2.2964  data: 0.9803\n",
      "Epoch: [96] Total time: 0:00:41 (2.2967 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 2.9811 (2.9705)  class_acc: 0.3633 (0.3683)  weight_decay: 0.0500 (0.0500)  grad_norm: 32.7239 (33.2576)\n",
      "Epoch: [97]  [ 0/18]  eta: 0:00:53  lr: 0.000010  min_lr: 0.000010  loss: 2.9991 (2.9991)  class_acc: 0.3398 (0.3398)  weight_decay: 0.0500 (0.0500)  time: 2.9696  data: 0.9746\n",
      "Epoch: [97]  [17/18]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.9588 (2.9528)  class_acc: 0.3594 (0.3678)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9597 (33.4572)  time: 2.1827  data: 0.9735\n",
      "Epoch: [97] Total time: 0:00:39 (2.1828 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 2.9588 (2.9528)  class_acc: 0.3594 (0.3678)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.9597 (33.4572)\n",
      "Epoch: [98]  [ 0/18]  eta: 0:01:01  lr: 0.000010  min_lr: 0.000010  loss: 3.0598 (3.0598)  class_acc: 0.3320 (0.3320)  weight_decay: 0.0500 (0.0500)  time: 3.4029  data: 0.9906\n",
      "Epoch: [98]  [17/18]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.9834 (2.9730)  class_acc: 0.3594 (0.3611)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.0178 (33.9832)  time: 2.2146  data: 0.9713\n",
      "Epoch: [98] Total time: 0:00:39 (2.2147 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 2.9834 (2.9730)  class_acc: 0.3594 (0.3611)  weight_decay: 0.0500 (0.0500)  grad_norm: 34.0178 (33.9832)\n",
      "Epoch: [99]  [ 0/18]  eta: 0:00:41  lr: 0.000010  min_lr: 0.000010  loss: 2.9501 (2.9501)  class_acc: 0.4023 (0.4023)  weight_decay: 0.0500 (0.0500)  time: 2.3245  data: 0.9732\n",
      "Epoch: [99]  [17/18]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.9431 (2.9425)  class_acc: 0.3789 (0.3798)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0457 (33.1596)  time: 2.2613  data: 0.9661\n",
      "Epoch: [99] Total time: 0:00:40 (2.2615 s / it)\n",
      "Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 2.9431 (2.9425)  class_acc: 0.3789 (0.3798)  weight_decay: 0.0500 (0.0500)  grad_norm: 33.0457 (33.1596)\n",
      "Training time 1:08:29\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 256 --epochs 100  --model rcvit_xs --data_path data/cub100/ --nb_classes 100 --lr 1e-4 --min_lr 1e-5 --weight_decay 0.05 --output_dir data/cub_ada/ --finetune model/cas-vit-xs.pth --warmup_epochs 3 --data_set image_folder  --adapter True --device mps  --model_ema_eval False --model_ema False --disable_eval True --clip_grad 2.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste do cub com Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 100\n",
      "Finetune resume checkpoint: data/cub_ada/checkpoint-99.pth\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.65      0.50        17\n",
      "           1       0.50      0.23      0.32        13\n",
      "           2       1.00      0.15      0.27        13\n",
      "           3       0.15      0.40      0.22         5\n",
      "           4       0.00      0.00      0.00         8\n",
      "           5       0.50      0.22      0.31         9\n",
      "           6       0.22      0.62      0.32         8\n",
      "           7       0.10      0.17      0.12         6\n",
      "           8       0.33      0.10      0.15        10\n",
      "           9       0.57      0.33      0.42        12\n",
      "          10       0.50      0.11      0.18         9\n",
      "          11       0.40      0.67      0.50         6\n",
      "          12       0.35      0.67      0.46         9\n",
      "          13       0.62      0.57      0.59        14\n",
      "          14       0.46      0.60      0.52        10\n",
      "          15       0.77      0.71      0.74        14\n",
      "          16       0.50      0.45      0.48        11\n",
      "          17       0.67      0.80      0.73        10\n",
      "          18       0.33      0.71      0.45         7\n",
      "          19       0.00      0.00      0.00        13\n",
      "          20       0.80      0.31      0.44        13\n",
      "          21       0.50      0.38      0.43         8\n",
      "          22       0.14      0.11      0.12         9\n",
      "          23       0.56      0.56      0.56         9\n",
      "          24       0.33      0.07      0.12        14\n",
      "          25       0.00      0.00      0.00        14\n",
      "          26       0.33      0.08      0.12        13\n",
      "          27       0.76      0.87      0.81        15\n",
      "          28       0.19      0.27      0.22        11\n",
      "          29       0.06      0.20      0.09         5\n",
      "          30       0.07      0.10      0.08        10\n",
      "          31       0.12      0.10      0.11        10\n",
      "          32       0.13      0.20      0.16        10\n",
      "          33       0.64      0.75      0.69        12\n",
      "          34       0.56      0.50      0.53        10\n",
      "          35       0.50      1.00      0.67         6\n",
      "          36       0.33      0.50      0.40         8\n",
      "          37       0.33      0.22      0.27         9\n",
      "          38       0.12      0.09      0.11        11\n",
      "          39       0.33      0.45      0.38        11\n",
      "          40       0.36      0.40      0.38        10\n",
      "          41       0.46      0.75      0.57         8\n",
      "          42       0.40      0.29      0.33        14\n",
      "          43       0.32      0.75      0.44         8\n",
      "          44       0.25      0.10      0.14        10\n",
      "          45       0.58      0.64      0.61        11\n",
      "          46       0.33      0.60      0.43        10\n",
      "          47       0.67      0.57      0.62        14\n",
      "          48       0.25      0.18      0.21        11\n",
      "          49       0.14      0.08      0.10        13\n",
      "          50       0.12      0.08      0.10        12\n",
      "          51       0.31      0.62      0.42         8\n",
      "          52       0.31      0.33      0.32        15\n",
      "          53       0.40      0.36      0.38        11\n",
      "          54       0.64      0.60      0.62        15\n",
      "          55       0.64      0.90      0.75        10\n",
      "          56       0.58      0.88      0.70         8\n",
      "          57       0.50      0.50      0.50         4\n",
      "          58       0.13      0.33      0.19         6\n",
      "          59       0.14      0.10      0.12        10\n",
      "          60       0.56      0.31      0.40        16\n",
      "          61       1.00      0.07      0.12        15\n",
      "          62       0.64      0.58      0.61        12\n",
      "          63       0.18      0.20      0.19        10\n",
      "          64       0.00      0.00      0.00         7\n",
      "          65       0.20      0.67      0.31         6\n",
      "          66       0.36      0.44      0.40         9\n",
      "          67       0.43      0.25      0.32        12\n",
      "          68       0.36      0.40      0.38        10\n",
      "          69       0.73      0.73      0.73        11\n",
      "          70       0.67      0.18      0.29        11\n",
      "          71       0.20      0.21      0.21        14\n",
      "          72       0.83      0.71      0.77         7\n",
      "          73       0.60      0.55      0.57        11\n",
      "          74       0.62      0.80      0.70        10\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.80      0.67      0.73        12\n",
      "          77       0.17      0.33      0.22         6\n",
      "          78       0.25      0.09      0.13        11\n",
      "          79       0.00      0.00      0.00        12\n",
      "          80       0.42      0.83      0.56         6\n",
      "          81       0.09      0.14      0.11         7\n",
      "          82       0.38      0.23      0.29        13\n",
      "          83       0.19      0.75      0.30         4\n",
      "          84       0.70      0.58      0.64        12\n",
      "          85       0.23      0.38      0.29         8\n",
      "          86       0.86      0.86      0.86         7\n",
      "          87       0.64      1.00      0.78         7\n",
      "          88       0.46      0.67      0.55         9\n",
      "          89       0.40      0.36      0.38        11\n",
      "          90       0.20      0.12      0.15         8\n",
      "          91       0.45      0.45      0.45        11\n",
      "          92       0.45      0.38      0.42        13\n",
      "          93       0.57      0.73      0.64        11\n",
      "          94       0.78      0.58      0.67        12\n",
      "          95       0.56      0.71      0.62         7\n",
      "          96       0.25      0.11      0.15         9\n",
      "          97       0.47      0.47      0.47        15\n",
      "          98       0.45      0.62      0.53         8\n",
      "          99       0.64      0.69      0.67        13\n",
      "\n",
      "    accuracy                           0.40      1024\n",
      "   macro avg       0.41      0.42      0.38      1024\n",
      "weighted avg       0.42      0.40      0.38      1024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 256 --model rcvit_xs --data_path data/cub100/ --nb_classes 100 --finetune data/cub_ada/checkpoint-99.pth   --data_set image_folder  --adapter True --device {device}  --test True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste do cub sem Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Number of the class = 100\n",
      "Finetune resume checkpoint: data/cub_fine/checkpoint-99.pth\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.71      0.65        17\n",
      "           1       0.62      0.38      0.48        13\n",
      "           2       0.27      0.23      0.25        13\n",
      "           3       0.36      0.80      0.50         5\n",
      "           4       0.60      0.38      0.46         8\n",
      "           5       0.86      0.67      0.75         9\n",
      "           6       0.50      0.75      0.60         8\n",
      "           7       0.30      0.50      0.38         6\n",
      "           8       0.38      0.30      0.33        10\n",
      "           9       0.67      0.67      0.67        12\n",
      "          10       0.40      0.44      0.42         9\n",
      "          11       0.83      0.83      0.83         6\n",
      "          12       0.64      0.78      0.70         9\n",
      "          13       0.87      0.93      0.90        14\n",
      "          14       0.75      0.60      0.67        10\n",
      "          15       1.00      0.79      0.88        14\n",
      "          16       0.80      0.73      0.76        11\n",
      "          17       0.82      0.90      0.86        10\n",
      "          18       0.40      0.57      0.47         7\n",
      "          19       0.89      0.62      0.73        13\n",
      "          20       0.50      0.38      0.43        13\n",
      "          21       0.89      1.00      0.94         8\n",
      "          22       0.00      0.00      0.00         9\n",
      "          23       0.67      0.67      0.67         9\n",
      "          24       0.71      0.36      0.48        14\n",
      "          25       0.43      0.43      0.43        14\n",
      "          26       0.75      0.23      0.35        13\n",
      "          27       0.93      0.87      0.90        15\n",
      "          28       0.33      0.36      0.35        11\n",
      "          29       0.13      0.40      0.20         5\n",
      "          30       0.29      0.40      0.33        10\n",
      "          31       0.42      0.50      0.45        10\n",
      "          32       0.22      0.20      0.21        10\n",
      "          33       0.89      0.67      0.76        12\n",
      "          34       0.86      0.60      0.71        10\n",
      "          35       0.75      1.00      0.86         6\n",
      "          36       0.23      0.38      0.29         8\n",
      "          37       0.33      0.44      0.38         9\n",
      "          38       0.43      0.27      0.33        11\n",
      "          39       0.42      0.45      0.43        11\n",
      "          40       0.78      0.70      0.74        10\n",
      "          41       0.62      1.00      0.76         8\n",
      "          42       0.62      0.36      0.45        14\n",
      "          43       0.46      0.75      0.57         8\n",
      "          44       0.38      0.30      0.33        10\n",
      "          45       0.82      0.82      0.82        11\n",
      "          46       0.73      0.80      0.76        10\n",
      "          47       0.87      0.93      0.90        14\n",
      "          48       0.36      0.36      0.36        11\n",
      "          49       0.67      0.15      0.25        13\n",
      "          50       0.56      0.83      0.67        12\n",
      "          51       0.64      0.88      0.74         8\n",
      "          52       0.93      0.87      0.90        15\n",
      "          53       0.86      0.55      0.67        11\n",
      "          54       0.81      0.87      0.84        15\n",
      "          55       0.62      0.80      0.70        10\n",
      "          56       0.78      0.88      0.82         8\n",
      "          57       0.44      1.00      0.62         4\n",
      "          58       0.07      0.17      0.10         6\n",
      "          59       0.40      0.40      0.40        10\n",
      "          60       0.71      0.62      0.67        16\n",
      "          61       0.67      0.13      0.22        15\n",
      "          62       0.82      0.75      0.78        12\n",
      "          63       0.42      0.50      0.45        10\n",
      "          64       0.40      0.29      0.33         7\n",
      "          65       0.15      0.33      0.21         6\n",
      "          66       0.50      0.89      0.64         9\n",
      "          67       0.71      0.42      0.53        12\n",
      "          68       0.67      0.60      0.63        10\n",
      "          69       0.85      1.00      0.92        11\n",
      "          70       0.43      0.27      0.33        11\n",
      "          71       0.75      0.43      0.55        14\n",
      "          72       1.00      1.00      1.00         7\n",
      "          73       0.75      0.82      0.78        11\n",
      "          74       0.90      0.90      0.90        10\n",
      "          75       0.70      0.64      0.67        11\n",
      "          76       0.83      0.83      0.83        12\n",
      "          77       0.42      0.83      0.56         6\n",
      "          78       0.55      0.55      0.55        11\n",
      "          79       0.75      0.75      0.75        12\n",
      "          80       0.75      1.00      0.86         6\n",
      "          81       0.20      0.14      0.17         7\n",
      "          82       1.00      0.77      0.87        13\n",
      "          83       0.23      0.75      0.35         4\n",
      "          84       0.77      0.83      0.80        12\n",
      "          85       0.54      0.88      0.67         8\n",
      "          86       0.71      0.71      0.71         7\n",
      "          87       0.67      0.86      0.75         7\n",
      "          88       1.00      0.89      0.94         9\n",
      "          89       0.88      0.64      0.74        11\n",
      "          90       0.50      0.25      0.33         8\n",
      "          91       0.75      0.82      0.78        11\n",
      "          92       0.73      0.62      0.67        13\n",
      "          93       0.92      1.00      0.96        11\n",
      "          94       0.88      0.58      0.70        12\n",
      "          95       0.36      0.71      0.48         7\n",
      "          96       0.50      0.22      0.31         9\n",
      "          97       0.78      0.47      0.58        15\n",
      "          98       0.67      0.75      0.71         8\n",
      "          99       0.85      0.85      0.85        13\n",
      "\n",
      "    accuracy                           0.61      1024\n",
      "   macro avg       0.62      0.62      0.60      1024\n",
      "weighted avg       0.65      0.61      0.61      1024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python main.py --batch_size 256 --model rcvit_xs --data_path data/cub100/ --nb_classes 100 --finetune data/cub_fine/checkpoint-99.pth  --data_set image_folder  --adapter False --device {device}  --test True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Meu Dataset*** :\n",
    "Compostos por 3 classes.\n",
    "\n",
    "- Parametros Treinaveis com Adapter:  134011\n",
    "\n",
    "- Parametros Treinaveis sem Adapter: 2759363\n",
    "\n",
    "- Diferenca: Adapter tem 4.85% de parametros em relacao a sem\n",
    "\n",
    "***CUB100***: É o Dataset CUB com apenas 100 classes. Eu alterei, pois o dataset demorava muito para completar cada ciclo de treinamento. \n",
    "\n",
    "- Parametros Treinaveis com Adapter:  155448\n",
    "\n",
    "- Parametros Treinaveis sem Adapter: 2802900\n",
    "\n",
    "- Diferenca: Adapter tem 5.54% de parametros em relacao a sem\n",
    "\n",
    "***Treinamento***\n",
    "- Feito em 20 epocas com o meu dataset\n",
    "- Feito em 100 epocas com o dataset CUB\n",
    "- Apenas o treinamento de finetuning no CUB foi realizado no google colab, pois a maquina leva muito tempo.\n",
    "- Resto do treinamento foi realizado em um MacBook com M2 pro.\n",
    "\n",
    "***Link dos Datasets estao nas celulas anteriores***\n",
    "-  url = 'https://drive.google.com/uc?id=1mEKi61CNbla7KonRU1UYtN6NS9f5yHvc'\n",
    "-  url2 = 'https://drive.google.com/uc?id=1bGuV5-gR2gSQOiaSB3T5zvTRBEFe1UTd'\n",
    "\n",
    "***Desempenho no Teste do Meu Dataset***\n",
    "- Com Adapters teve um melhor desempenho, provavelmente pois o finetuning era mais complexo para generalizar.\n",
    "\n",
    "***Desempenho no Teste do CUB***\n",
    "\n",
    "- O Finetuning obteve um desempenho muito melhor em relacao aos adapters, valor saturou em 40% com Adapters.\n",
    "- O modelo com Finetuning nao obter um valor maior ja era esperado, pois eu peguei o modelo mais simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sun-hailong/CVPR24-Ease/blob/main/resources/result-img.png?raw=true\" alt=\"Exemplo de Imagem\" width=\"900\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
